{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione\n",
    "- **Classificare** significa identificare a quale classe appartiene un oggetto (in gergo *etichettare*), sulla base di un insieme di **caratteristiche** (features) osservate.\n",
    "\n",
    "- Ad esempio date delle cellule tumorali aventi determinate caratteristiche\n",
    "    \n",
    "    - area\n",
    "    - perimetro\n",
    "    - consistenza\n",
    "    - varianza scala di grigi\n",
    "    - ...\n",
    "    \n",
    "  si vuole classificare se sono **benigne** o **maligne**\n",
    "\n",
    "- Sulla base di queste features, si possono plottare i dati relativi su un grafico\n",
    "\n",
    "<img src=\"imgs/classificazione.PNG\" alt=\"\" width=500>\n",
    "\n",
    "- Il grafico in questione, per essere compreso facilmente, ha solo due feature rappresentate, ma in realtà ce ne possono essere molte di più.\n",
    "\n",
    "- In questo caso, sulla base delle feature, le cellule possono essere etichettate in due classi\n",
    "  \n",
    "  - **benigne** (punti blu)\n",
    "  - **maligne** (punti rossi)\n",
    "\n",
    "- In questo senso la **classificazione** corrisponde all'*identificazione di una funzione* che massimizzi la separazione fra le due classi.\n",
    "  \n",
    "  - in figura la retta nera\n",
    "\n",
    "## Iperpiani di Separazione e Classificazione\n",
    "- Si parla dunque di ricerca di **iperpiani di separazione** (o di **classificazione**) fra le classi.\n",
    "\n",
    "<img src=\"imgs/iperpiano.PNG\" alt=\"\" width=350>\n",
    "\n",
    "- La ricerca di iperpiani di separazione è, fra l'altro, un problema ricorrente all'interno delle reti neurali. \n",
    "\n",
    "- Dati i punti $\\mathbf{x}_i$ (è un vettore poiché generalmente ogni punto può essere identificato da più features) che giaciono sull'iperpiano, soddisfano la seguente uguaglianza (equazione dell'iperpiano) $\\mathbf{w}^T \\cdot \\mathbf{x}_i + b = 0$\n",
    "\n",
    "- I punti $x_i$ tali che $\\mathbf{w}^T \\cdot \\mathbf{x}_i > b$ sono le istanze con $\\mathbf{y}_i > 0$ \n",
    "  \n",
    "  - come nella regressione la variabile target (da predire) è la $\\mathbf{y}$\n",
    "    \n",
    "    - poiché in questo caso stiamo classificando le istanze in due valori, la variabile può assumere solo due valori (-1 o 1).\n",
    "  \n",
    "  - queste istanze vengono etichettate con label 1\n",
    "\n",
    "- I punti $\\mathbf{x}_i$ tali che $\\mathbf{w}^T \\cdot \\mathbf{x}_i < -b$ sono le istanze con $\\mathbf{y}_i < 0$ \n",
    "  \n",
    "  - queste istanze vengono etichettate con label -1\n",
    "\n",
    "- Il termine $b$ (intercetta), è proporzionale alla distanza dell'iperpiano dall'origine.\n",
    "  \n",
    "  - Infatti la distanza dell'iperpiano dall'origine misura \n",
    "  $$\n",
    "    d = \\frac{b}{\\vert\\vert \\mathbf{w} \\vert \\vert}\n",
    "  $$\n",
    "\n",
    "## Caratteristche dei classificatori\n",
    "- Gli iperpiani di classificazione possono essere molteplici\n",
    "  \n",
    "  - potenzialmente possono essere anche infiniti (magari non tutti con stesso **errore**)\n",
    "    \n",
    "    - l'**errore** nel caso di classificazione è rappresentato da un'etichettatura errata.\n",
    "\n",
    "<img src=\"imgs/iperpiani.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Occorre pertanto stabilire un criterio di **ottimalità** per la scelta dell'iperpiano di separazione migliore.\n",
    "\n",
    "- Sulla base di questo criterio si possono costruire dei modelli di ricerca di questi iperpiani.\n",
    "  \n",
    "  - Alcuni modelli individuano un iperpiano di separazione **non ottimale** (non massimizza la separazione fra le classi), ma che comunque separa le classi.\n",
    "    \n",
    "    - Alcuni di questi modelli sono **Perceptron**, **Regressione Logisitca**, ecc.\n",
    "  \n",
    "  - Altri modelli individuano un iperpiano di separazione **ottimale** (massimizza la separazione fra le classi).\n",
    "    \n",
    "    - Il più famoso è il **Support Vector Machine** (SVM).  \n",
    "\n",
    "- Quali dati influenzano la ricerca dell'iperpiano?\n",
    "  \n",
    "  - Gli algoritmi più banali (Perceptron, Regressione Logistica, ecc.) utilizzano tutti i punti del training set.\n",
    "  \n",
    "  - Altri algoritmi (come SVM) usano i **punti difficili** \n",
    "    \n",
    "    - sono i punti più vicini all'iperpiano di separazione \n",
    "    \n",
    "    - d'altronde se riesco a trovare un iperpiano che suddivide correttamente quei punti, quelli più distanti saranno classificati correttamente di conseguenza\n",
    "\n",
    "\n",
    "## Classificatori non lineari\n",
    "- Non tutti i problemi possono essere separati linearmente (cioè da un iperpiano).\n",
    "\n",
    "<img src=\"imgs/classificatorinonlineari.PNG\" alt=\"\" width=200>\n",
    "\n",
    "- In questi casi esistono numerosi approcci che risolvono il problema \n",
    "  \n",
    "  - **Algoritmi non lineari** (es. SVM, Reti Neurali, k-Nearest-Neighbors, ecc.)\n",
    "  \n",
    "  - Queste oluzioni **trasformano lo spazio di dati** in modo che le classi siano **separabili linearmente**\n",
    "\n",
    "- Supponiamo di trovarci in un caso monodimensionale: una sola feature con cui prevedere la classe.\n",
    "  \n",
    "  - I dati pertanto sono disposti su una retta come mostato in figura (rossi di una classe, blu dell'altra).\n",
    "  \n",
    "  - In questo caso un classificatore non riuscirebbe a separare linearmente i dati\n",
    "    \n",
    "    - dovrebbe tracciare una retta che divide lo spazio in due semipiani ma nessuno dei due conterrà solo dati di una classe\n",
    "\n",
    "- Si può pensare in tal caso di trasformare lo spazio vettoriale da una dimensione a due\n",
    "  \n",
    "  - cioè aggiungendo una nuova feature\n",
    "  \n",
    "  - i dati sono gli stessi rispetto a prima ma sono disposti su due dimensioni\n",
    "    \n",
    "- In questo nuovo spazio bidimensionale, esiste un iperpiano che riesce a separare le due classi linearmente.\n",
    "\n",
    "<img src=\"imgs/classificatorinonlineari2.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Le **reti neurali** sono composte da layer che non fanno altro che trasformare lo spazio.\n",
    "  \n",
    "  - La differenza da un algoritmo di classificazione (es. SVM) è che la trasformazione non è nota a priori, ma viene appresa durante il training.\n",
    "  \n",
    "  - Nelle SVM definiamo noi la trasformazione da fare, nel caso sopra una trasformazione polinomiale\n",
    "  \n",
    "  - Le reti neurali sanno determinare autonomamente le feature\n",
    "\n",
    "- Più feature si hanno nei dati, e più chance abbiamo di separare linearmente i dati.\n",
    "  \n",
    "  - Intuitivamente maggiore è la dimensionalità dello spazio e più facile è trovare un iperpiano che separa le classi (si è visto nell'esempio sopra trasformando lo spazio $\\mathbb{R}$ in $\\mathbb{R}^2$).\n",
    "\n",
    "# Classificatore Lineare: Perceptron\n",
    "- Primo classificatore lineare inventato. \n",
    "\n",
    "  - Nasce nel 1957 da Frank Rosenblatt\n",
    "\n",
    "- Supponiamo di avere uno spazio in $\\mathbb{R}^2$ \n",
    "\n",
    "  - Abbiamo due features $x_1, x_2$ che consentono di riportare sullo spazio (un piano) tutti i dati, come mostrato in figura\n",
    "\n",
    "<img src=\"imgs/perceptron.PNG\" alt=\"\" width=400>\n",
    "\n",
    "- In tal caso la classificazione consiste nel trovare un iperpiano (in $\\mathbb{R}^2$ corrisponde a una retta) che separi le due classi.\n",
    "\n",
    "  - Occorre cioè trovarer i parametri $w_1, w_2, b$ tali che\n",
    "    $$\n",
    "      \\begin{align}\n",
    "        &w_1x_1 + w_2x_2 + b > 0 \\quad \\text{per i punti rossi} \\\\\n",
    "        &w_1x_1 + w_2x_2 + b < 0 \\quad \\text{per i punti blu}\n",
    "      \\end{align}\n",
    "    $$\n",
    "\n",
    "    - In tal caso le etichette saranno definite in questo modo\n",
    "    $$\n",
    "      \\text{class}(x) = \\begin{cases}\n",
    "        +1 \\text{ se } x \\text{ è rosso} \\\\\n",
    "        -1 \\text{ se } x \\text{ è blu}\n",
    "      \\end{cases} \n",
    "    $$\n",
    "\n",
    "- Il Perceptron è un classificatore lineare che trova l'iperpiano di separazione in modo iterativo.\n",
    "\n",
    "  - Di seguito lo pseudocodice\n",
    "\n",
    "  ```python\n",
    "    def perceptron_naive(x):\n",
    "      # valori per b, w1, w2 generati casualmente, \n",
    "      # prendiamo cioè un piano casuale (in figura sopra la prima linea verde)\n",
    "      # w1x1 + w2x2 + b = 0 -> -x1 + 9x2 = 0 -> x2 = -w1/w2 * x1 -> x2 = 0.11x1\n",
    "      w = [0,-1,9] \n",
    "      x0 = 1\n",
    "      classification_error = False\n",
    "      for each instance x:\n",
    "        if class(x) < 0 and w * x >= 0: # è un errore perché la classe è -1 ma il prodotto è >= 0 (cioè è classificato come +1)\n",
    "          classification_error = True\n",
    "          w = w - x # si sposta l'iperpiano più in alto\n",
    "        else if class(x) > 0 and w * x <= 0: # è un errore perché la classe è +1 ma il prodotto è <= 0 (cioè è classificato come -1)\n",
    "          classification_error = True\n",
    "          w = w + x\n",
    "      until classification_error == True\n",
    "  \n",
    "  ```\n",
    "\n",
    "  l'idea è quella di ripetere il ciclo finché non ho più errore di classificazione, in modo da trovare l'iperpiano di separazione.\n",
    "     \n",
    "     - Alla prima iterazione del ciclo (in figura il quadratro numero 1) si verifica un errore poiché la retta classifica il punto cerchiato in verde in figura come > 0 (cioè sopra l'iperpiano), tuttavia il pallino è blu e dunque dovrebbe stare sotto la retta, dunque si fa $\\mathbf{w} = \\mathbf{w} - \\mathbf{x}$ (in figura la seconda linea verde)\n",
    "\n",
    "     - Alla seconda iterazione (in figura il riquadro numero due 2), si verifica lo stesso errore poiché la retta classifica ancora il punto come > 0, dunque si sposta di nuovo la figura più in alto\n",
    "\n",
    "     - Alla terza iterazione il punto cerchiato in nero questa volta ha lo stesso problema di prima: è classificato come > 0 anche se blu. Rispostando la figura più in alto si risolve ottenendo la retta nera di valori $\\mathbf{w}_1 = -6, \\mathbf{w}_2 = 6, b = 2$.\n",
    "\n",
    "  - Questa implementazione base converge solamente se i dati sono linearmente separabili.\n",
    "    \n",
    "    - L'errore non potrà mai essere 0 qualora i dati non siano separabili, dunque il ciclo non terminerà mai.\n",
    "\n",
    "  - È il progenitore delle **reti neurali**\n",
    "    - non a caso esiste un modello **multi-layer perceptron** che è una rete neurale su cui ogni layer è un perceptron messo a cascata\n",
    "\n",
    "\n",
    "- Usando l'esempio di partenza sulle cellule tumorali \n",
    "\n",
    "  - siamo in $\\mathbb{R}^2$ (2 features)\n",
    "    \n",
    "    - $x_1 = mean_area$\n",
    "    - $x_2 = mean_concave_points$\n",
    "\n",
    "  - si vuole classificare se sono **benigne** o **maligne**\n",
    "\n",
    "    - definiamo una variabile target $y$ tale che\n",
    "    $$\n",
    "    y = \\begin{cases}\n",
    "      -1 \\text{ se la cellula è benigna} \\\\\n",
    "      +1 \\text{ se la cellula è maligna}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "  - La separazione delle classi si effettua tramite un iperpiano (che in questo caso è una retta) definito da\n",
    "    $$\n",
    "      w_1x_1 + w_2x_2 + b = 0\n",
    "    $$\n",
    "\n",
    "  - dove $w_1, w_2, b$ sono i parametri da stimare in modo tale che\n",
    "    \n",
    "    - se $w_1x_1 + w_2x_2 + b > 0$ allora $y = +1$, cioè la cellula è maligna\n",
    "\n",
    "    - se $w_1x_1 + w_2x_2 + b < 0$ allora $y = -1$, cioè la cellula è benigna\n",
    "\n",
    "  - ovvero\n",
    "    $$\n",
    "    y = \\begin{cases}\n",
    "    -1 \\quad \\text{if } \\mathbf{w \\cdot x} + b < 0 \\\\\n",
    "    +1 \\quad \\text{if } \\mathbf{w \\cdot x} + b \\geq 0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "\n",
    "  - Supponiamo che l'algoritmo abbia trovato la seguente retta \n",
    "    $$\n",
    "    0.007 \\cdot mean\\_area + 67.443 \\cdot mean\\_concave\\_points - 8.287 = 0\n",
    "    $$\n",
    "\n",
    "  - la classificazione di due nuove cellule sarà semplice\n",
    "\n",
    "    - $y^{(1)} = \\mathbf{x}^{(1)} = (mean\\_area = 500, mean\\_concave\\_points = 0.025)$ da cui otterremo \n",
    "    $$\n",
    "    \\mathbf{w \\cdot x}^{(1)} + b = 0.007 \\cdot 500 + 67.443 \\cdot 0.025 - 8.287 = -3.035\n",
    "    $$\n",
    "    dunque una cellula benigna\n",
    "\n",
    "    - $y^{(2)} = \\mathbf{x}^{(2)} = (mean\\_area = 500, mean\\_concave\\_points = 0.075)$ da cui otterremo\n",
    "    $$\n",
    "    \\mathbf{w \\cdot x}^{(2)} + b = 0.007 \\cdot 500 + 67.443 \\cdot 0.075 - 8.287 = 0.271\n",
    "    $$\n",
    "    è una cellula maligna ma molto boarder line in quanto molto vicina all'iperpiano.\n",
    "\n",
    "\n",
    "## Come trovare l'iperpiano di separazione\n",
    "- Si può riscrivere la separazione con etichette\n",
    "  $$\n",
    "      y = \\begin{cases}\n",
    "      -1 \\quad \\text{if } \\mathbf{w \\cdot x} + b < 0 \\\\\n",
    "      +1 \\quad \\text{if } \\mathbf{w \\cdot x} + b \\geq 0\n",
    "      \\end{cases}\n",
    "  $$\n",
    "  come $-y(b+ \\mathbf{w \\cdot x}) < 0$.\n",
    "  \n",
    "  - Infatti una cellula classificata correttamente (indipendentemente dalla classe di appartenenza), con questa forma compatta darà valore negativo, mentre una cellula classificata erroneamente darà valore positivo.\n",
    "\n",
    "    - ad esempio se $y=-1$ (cellula benigna), $b+\\mathbf{w \\cdot x} = -1$ (cioè la valutazione sulla base delle feature $\\mathbf{x}$ e ell'iperpiano $\\mathbf{w}$ è corretta) allora $-y(b+\\mathbf{w \\cdot x}) = 1 \\cdot -1 < 0$ (classificata correttamente)\n",
    "    - se invece $y=1$ (cellula maligna), nonostante $b+\\mathbf{w \\cdot x} = -1$ (cioè la valutazione sulla base delle feature $\\mathbf{x}$ e ell'iperpiano $\\mathbf{w}$ è errata) allora $-y(b+\\mathbf{w \\cdot x}) = -1 \\cdot -1 > 0$ (classificata erroneamente)\n",
    "\n",
    "- A questo punto minimizzando rispetto a $b$ e $\\mathbf{w}$ la somma di questa espressione sulle $m$ istanze di training, si minimizza l'errore di classificazione.\n",
    "  $$\n",
    "    \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\text{max}(0, -y_i(b + \\mathbf{w \\cdot x}_i))\n",
    "  $$\n",
    "  \n",
    "  - considerando che $-y(b+ \\mathbf{w \\cdot x}) < 0$ è equivalente a $\\text{max}(0, -y_i(b + \\mathbf{w \\cdot x}_i))$.\n",
    "\n",
    "### Come minimizzare la funzione: softmax\n",
    "- La funzione appena mostrata (visibile nella figura sottostante in verde) è però non derivabile, poiché in 0 ha uno spigolo, dunque non si può applicare la discesa del gradiente per minimizzarla.\n",
    "\n",
    "<img src=\"imgs/iperpianofunzione.PNG\" alt=\"\" width=350>\n",
    "\n",
    "- L'idea è quella di trovare una funzione che la approssimi (in figura tratteggiata) e che sia derivabile.\n",
    "  \n",
    "  - Definendo la funzione da minimizzare come $\\text{max}(0, s)$, la funzione che la approssima, chiamata **softmax** (il nome deriva dal fatto che è una versione smooth (soft) della max), è definita come segue\n",
    "  $$\n",
    "  \\text{softmax}(0,s) = \\log \\left( 1 + e^s \\right)\n",
    "  $$ \n",
    "    \n",
    "    - questa funzione è covessa, continua e derivabile\n",
    "\n",
    "  - Minimizzando la somma rispetto a $b$ e $\\mathbf{w}$ della softmax su tutte le istanze, si minimizza l'errore di classificazione.\n",
    "  $$\n",
    "    \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\log \\left( 1 + e^{-y_i(b + \\mathbf{w \\cdot x}_i)} \\right)\n",
    "  $$\n",
    "\n",
    "\n",
    "## Regressione logistica\n",
    "- Per la **regressione logistica** si minimizza la funzione data dalla somma della softmax e la **regolarizzazione** L2 dei parametri con peso $\\lambda$\n",
    "\n",
    "$$\n",
    "  \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\log \\left( 1 + e^{-y_i(b + \\mathbf{w \\cdot x}_i)} \\right) + \\frac{\\lambda}{2} \\vert \\vert \\mathbf{w} \\vert \\vert^2_2\n",
    "$$\n",
    "\n",
    "  - Il motivo per cui si aggiunge la regolarizzazione è che senza di essa l'errore minimo trovato può anche essere molto basso per il training set, ma molto elevato per il test set, in altre parole, minimizzare un errore non regolarizzato può portare a dell'**overfitting**.\n",
    "\n",
    "  - Ponendo questo ulteriore vincolo, imponiamo che l'errore giaccia su una circonferenza centrata sull'origine.\n",
    "\n",
    "    - Ci allontaneremo dal minimo globale, ma poiché saremo più vicini all'origine i valori di $w_1, w_2$ che sono i parametri che definiscono l'iperpiano saranno più piccoli in valore assoluto. Più piccoli sono i parametri e **meno oscilazioni ci sono nell'iperpiano**.\n",
    "\n",
    "<img src=\"imgs/regolarizzazione.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- L'iperpiano appreso facendo la discesa del gradiente ci consente di individuare una funzione che faccia da **classificatore lineare**, ossia una funzione che, dato un dato in input, stabilisca a quale classe appartiene.\n",
    "$$\n",
    "  \\sigma(h_{\\mathbf{\\tilde{w}}}(\\mathbf{\\tilde{x}})) = \\frac{1}{1 + e^{-h_{\\mathbf{\\tilde{w}}}(\\mathbf{x})}} = \\frac{1}{1 + e^{-\\mathbf{\\tilde{w}} \\cdot \\mathbf{\\tilde{x}}}} = \\frac{1}{1 + e^{-(\\mathbf{w} \\cdot \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "  - dove $h_{\\mathbf{\\tilde{w}}}(\\tilde{\\mathbf{x}}) = -y_i(b + \\mathbf{w \\cdot x}_i)$ e $b+ \\mathbf{w \\cdot x}$ è l'iperpiano individuato.\n",
    "\n",
    "- In conclusione la regressione logistica è una funzione che\n",
    "  \n",
    "  - dato il parametro in input $\\sigma(t) = \\frac{1}{1+e^{-(b+w\\cdot t)}}$ dove $t$ è l'istanza da classificare (ad esempio una nuova cellula)\n",
    "  \n",
    "  - restituisce un valore in $[0,1]$ che è la probabilità che l'istanza appartenga alla classe positiva (ad esempio maligna).\n",
    "    \n",
    "    - In altre parole la funzione è definita come $\\mathbb{R} \\rightarrow [0,1]$\n",
    " \n",
    "  - Questa funzione approssima una **funzione a gradino** \n",
    "    \n",
    "    - In particolare se $\\mathbf{x}$ giace sull'iperpiano allora $\\mathbf{w \\cdot x} + b = 0$ e dunque $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^0} = 0.5$, cioè non sappiamo se la cellula appartiene ad una classe o all'altra, poiché la probabilità che appartenga ad una rispetto all'altra è uguale (sempre 0.5).\n",
    "    - Se invece risulta $\\mathbf{w \\cdot x} + b > 0$ allora $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^{-(b + \\mathbf{w \\cdot x})> 0}} > 0.5$, cioè l'istanza appartiene alla prima classe.\n",
    "    - Se al contrario abbiamo un valore negativo, cioè $\\mathbf{w \\cdot x} + b < 0$ allora $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^{-(b + \\mathbf{w \\cdot x})< 0}} > 0.5$, classe (-1).\n",
    "\n",
    "<img src=\"imgs/regrlogistica.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Questi risultati possono essere generalizzati in maniera che valgano \n",
    "  - ovviamente per istanze con più di una sola feature (da qui il vettore $\\mathbf{x}$)\n",
    "  - Per più di due classi (da qui il vettore $\\mathbf{w}$)\n",
    "\n",
    "\n",
    "## Regressione logistica multivariata lineare e non lineare\n",
    "- Abbiamo già anticipato che è possibile generalizzare la regressione logistica per classificare istanse che non sono classificabili linearmente. \n",
    " \n",
    "  - Questo significa che lo spazio in cui sono rappresentate le istanze non ha un iperpiano che le suddivida in classi distinte senza errori.\n",
    "\n",
    "- Sia $\\mathbf{x} \\in \\mathbb{R}^n$ un'istanza $\\mathbf{x} = (x_1, \\dots, x_n)$ in $n$ variabili\n",
    "  \n",
    "  - la **funzione di classificazione lineare multivariata** è\n",
    "    $$\n",
    "    \\sigma(x_1, \\dots, x_n) = \\frac{1}{1+e^{-h_w(x_1, \\dots, x_n)}}\n",
    "    $$\n",
    "    \n",
    "    dove $h_w(x_1, \\dots, x_n) = b + w_1x_1 + \\dots + w_nx_n$ è l'**iperpiano di separazione**, chiaramente di dimensione $n$ se $n$ è il numero di features (o dimensione dello spazio che deve tagliare).\n",
    "\n",
    "    - l'individuazione di questo iperpiano avviene nella stessa modalità che abbiamo visto prima: minimizzando la funzione di softmax su tutte le istanze di training.\n",
    "\n",
    "- La **regressione logistica non lineare**, ad esempio con un polinomio di grado 2 avviene nel seguente modo\n",
    "$$\n",
    "  \\sigma(x_1, \\dots, x_n) = \\frac{1}{1+e^{h_w^2(x_1, \\dots, x_n)}} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
