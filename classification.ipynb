{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione\n",
    "- **Classificare** significa identificare a quale classe appartiene un oggetto (in gergo *etichettare*), sulla base di un insieme di **caratteristiche** (features) osservate.\n",
    "\n",
    "- Ad esempio date delle cellule tumorali aventi determinate caratteristiche\n",
    "    \n",
    "    - area\n",
    "    - perimetro\n",
    "    - consistenza\n",
    "    - varianza scala di grigi\n",
    "    - ...\n",
    "    \n",
    "  si vuole classificare se sono **benigne** o **maligne**\n",
    "\n",
    "- Sulla base di queste features, si possono plottare i dati relativi su un grafico\n",
    "\n",
    "<img src=\"imgs/classificazione.PNG\" alt=\"\" width=500>\n",
    "\n",
    "- Il grafico in questione, per essere compreso facilmente, ha solo due feature rappresentate, ma in realtà ce ne possono essere molte di più.\n",
    "\n",
    "- In questo caso, sulla base delle feature, le cellule possono essere etichettate in due classi\n",
    "  \n",
    "  - **benigne** (punti blu)\n",
    "  - **maligne** (punti rossi)\n",
    "\n",
    "- In questo senso la **classificazione** corrisponde all'*identificazione di una funzione* che massimizzi la separazione fra le due classi.\n",
    "  \n",
    "  - in figura la retta nera\n",
    "\n",
    "## Iperpiani di Separazione e Classificazione\n",
    "- Si parla dunque di ricerca di **iperpiani di separazione** (o di **classificazione**) fra le classi.\n",
    "\n",
    "<img src=\"imgs/iperpiano.PNG\" alt=\"\" width=350>\n",
    "\n",
    "- La ricerca di iperpiani di separazione è, fra l'altro, un problema ricorrente all'interno delle reti neurali. \n",
    "\n",
    "- Dati i punti $\\mathbf{x}_i$ (è un vettore poiché generalmente ogni punto può essere identificato da più features) che giaciono sull'iperpiano, soddisfano la seguente uguaglianza (equazione dell'iperpiano) $\\mathbf{w}^T \\cdot \\mathbf{x}_i + b = 0$\n",
    "\n",
    "- I punti $x_i$ tali che $\\mathbf{w}^T \\cdot \\mathbf{x}_i > b$ sono le istanze con $\\mathbf{y}_i > 0$ \n",
    "  \n",
    "  - come nella regressione la variabile target (da predire) è la $\\mathbf{y}$\n",
    "    \n",
    "    - poiché in questo caso stiamo classificando le istanze in due valori, la variabile può assumere solo due valori (-1 o 1).\n",
    "  \n",
    "  - queste istanze vengono etichettate con label 1\n",
    "\n",
    "- I punti $\\mathbf{x}_i$ tali che $\\mathbf{w}^T \\cdot \\mathbf{x}_i < -b$ sono le istanze con $\\mathbf{y}_i < 0$ \n",
    "  \n",
    "  - queste istanze vengono etichettate con label -1\n",
    "\n",
    "- Il termine $b$ (intercetta), è proporzionale alla distanza dell'iperpiano dall'origine.\n",
    "  \n",
    "  - Infatti la distanza dell'iperpiano dall'origine misura \n",
    "  $$\n",
    "    d = \\frac{b}{\\vert\\vert \\mathbf{w} \\vert \\vert}\n",
    "  $$\n",
    "\n",
    "## Caratteristche dei classificatori\n",
    "- Gli iperpiani di classificazione possono essere molteplici\n",
    "  \n",
    "  - potenzialmente possono essere anche infiniti (magari non tutti con stesso **errore**)\n",
    "    \n",
    "    - l'**errore** nel caso di classificazione è rappresentato da un'etichettatura errata.\n",
    "\n",
    "<img src=\"imgs/iperpiani.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Occorre pertanto stabilire un criterio di **ottimalità** per la scelta dell'iperpiano di separazione migliore.\n",
    "\n",
    "- Sulla base di questo criterio si possono costruire dei modelli di ricerca di questi iperpiani.\n",
    "  \n",
    "  - Alcuni modelli individuano un iperpiano di separazione **non ottimale** (non massimizza la separazione fra le classi), ma che comunque separa le classi.\n",
    "    \n",
    "    - Alcuni di questi modelli sono **Perceptron**, **Regressione Logisitca**, ecc.\n",
    "  \n",
    "  - Altri modelli individuano un iperpiano di separazione **ottimale** (massimizza la separazione fra le classi).\n",
    "    \n",
    "    - Il più famoso è il **Support Vector Machine** (SVM).  \n",
    "\n",
    "- Quali dati influenzano la ricerca dell'iperpiano?\n",
    "  \n",
    "  - Gli algoritmi più banali (Perceptron, Regressione Logistica, ecc.) utilizzano tutti i punti del training set.\n",
    "  \n",
    "  - Altri algoritmi (come SVM) usano i **punti difficili** \n",
    "    \n",
    "    - sono i punti più vicini all'iperpiano di separazione \n",
    "    \n",
    "    - d'altronde se riesco a trovare un iperpiano che suddivide correttamente quei punti, quelli più distanti saranno classificati correttamente di conseguenza\n",
    "\n",
    "\n",
    "## Classificatori non lineari\n",
    "- Non tutti i problemi possono essere separati linearmente (cioè da un iperpiano).\n",
    "\n",
    "<img src=\"imgs/classificatorinonlineari.PNG\" alt=\"\" width=200>\n",
    "\n",
    "- In questi casi esistono numerosi approcci che risolvono il problema \n",
    "  \n",
    "  - **Algoritmi non lineari** (es. SVM, Reti Neurali, k-Nearest-Neighbors, ecc.)\n",
    "  \n",
    "  - Queste oluzioni **trasformano lo spazio di dati** in modo che le classi siano **separabili linearmente**\n",
    "\n",
    "- Supponiamo di trovarci in un caso monodimensionale: una sola feature con cui prevedere la classe.\n",
    "  \n",
    "  - I dati pertanto sono disposti su una retta come mostato in figura (rossi di una classe, blu dell'altra).\n",
    "  \n",
    "  - In questo caso un classificatore non riuscirebbe a separare linearmente i dati\n",
    "    \n",
    "    - dovrebbe tracciare una retta che divide lo spazio in due semipiani ma nessuno dei due conterrà solo dati di una classe\n",
    "\n",
    "- Si può pensare in tal caso di trasformare lo spazio vettoriale da una dimensione a due\n",
    "  \n",
    "  - cioè aggiungendo una nuova feature\n",
    "  \n",
    "  - i dati sono gli stessi rispetto a prima ma sono disposti su due dimensioni\n",
    "    \n",
    "- In questo nuovo spazio bidimensionale, esiste un iperpiano che riesce a separare le due classi linearmente.\n",
    "\n",
    "<img src=\"imgs/classificatorinonlineari2.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Le **reti neurali** sono composte da layer che non fanno altro che trasformare lo spazio.\n",
    "  \n",
    "  - La differenza da un algoritmo di classificazione (es. SVM) è che la trasformazione non è nota a priori, ma viene appresa durante il training.\n",
    "  \n",
    "  - Nelle SVM definiamo noi la trasformazione da fare, nel caso sopra una trasformazione polinomiale\n",
    "  \n",
    "  - Le reti neurali sanno determinare autonomamente le feature\n",
    "\n",
    "- Più feature si hanno nei dati, e più chance abbiamo di separare linearmente i dati.\n",
    "  \n",
    "  - Intuitivamente maggiore è la dimensionalità dello spazio e più facile è trovare un iperpiano che separa le classi (si è visto nell'esempio sopra trasformando lo spazio $\\mathbb{R}$ in $\\mathbb{R}^2$).\n",
    "\n",
    "# Classificatore Lineare: Perceptron\n",
    "- Primo classificatore lineare inventato. \n",
    "\n",
    "  - Nasce nel 1957 da Frank Rosenblatt\n",
    "\n",
    "- Supponiamo di avere uno spazio in $\\mathbb{R}^2$ \n",
    "\n",
    "  - Abbiamo due features $x_1, x_2$ che consentono di riportare sullo spazio (un piano) tutti i dati, come mostrato in figura\n",
    "\n",
    "<img src=\"imgs/perceptron.PNG\" alt=\"\" width=400>\n",
    "\n",
    "- In tal caso la classificazione consiste nel trovare un iperpiano (in $\\mathbb{R}^2$ corrisponde a una retta) che separi le due classi.\n",
    "\n",
    "  - Occorre cioè trovarer i parametri $w_1, w_2, b$ tali che\n",
    "    $$\n",
    "      \\begin{align}\n",
    "        &w_1x_1 + w_2x_2 + b > 0 \\quad \\text{per i punti rossi} \\\\\n",
    "        &w_1x_1 + w_2x_2 + b < 0 \\quad \\text{per i punti blu}\n",
    "      \\end{align}\n",
    "    $$\n",
    "\n",
    "    - In tal caso le etichette saranno definite in questo modo\n",
    "    $$\n",
    "      \\text{class}(x) = \\begin{cases}\n",
    "        +1 \\text{ se } x \\text{ è rosso} \\\\\n",
    "        -1 \\text{ se } x \\text{ è blu}\n",
    "      \\end{cases} \n",
    "    $$\n",
    "\n",
    "- Il Perceptron è un classificatore lineare che trova l'iperpiano di separazione in modo iterativo.\n",
    "\n",
    "  - Di seguito lo pseudocodice\n",
    "\n",
    "  ```python\n",
    "    def perceptron_naive(x):\n",
    "      # valori per b, w1, w2 generati casualmente, \n",
    "      # prendiamo cioè un piano casuale (in figura sopra la prima linea verde)\n",
    "      # w1x1 + w2x2 + b = 0 -> -x1 + 9x2 = 0 -> x2 = -w1/w2 * x1 -> x2 = 0.11x1\n",
    "      w = [0,-1,9] \n",
    "      x0 = 1\n",
    "      classification_error = False\n",
    "      for each instance x:\n",
    "        if class(x) < 0 and w * x >= 0: # è un errore perché la classe è -1 ma il prodotto è >= 0 (cioè è classificato come +1)\n",
    "          classification_error = True\n",
    "          w = w - x # si sposta l'iperpiano più in alto\n",
    "        else if class(x) > 0 and w * x <= 0: # è un errore perché la classe è +1 ma il prodotto è <= 0 (cioè è classificato come -1)\n",
    "          classification_error = True\n",
    "          w = w + x\n",
    "      until classification_error == True\n",
    "  \n",
    "  ```\n",
    "\n",
    "  l'idea è quella di ripetere il ciclo finché non ho più errore di classificazione, in modo da trovare l'iperpiano di separazione.\n",
    "     \n",
    "     - Alla prima iterazione del ciclo (in figura il quadratro numero 1) si verifica un errore poiché la retta classifica il punto cerchiato in verde in figura come > 0 (cioè sopra l'iperpiano), tuttavia il pallino è blu e dunque dovrebbe stare sotto la retta, dunque si fa $\\mathbf{w} = \\mathbf{w} - \\mathbf{x}$ (in figura la seconda linea verde)\n",
    "\n",
    "     - Alla seconda iterazione (in figura il riquadro numero due 2), si verifica lo stesso errore poiché la retta classifica ancora il punto come > 0, dunque si sposta di nuovo la figura più in alto\n",
    "\n",
    "     - Alla terza iterazione il punto cerchiato in nero questa volta ha lo stesso problema di prima: è classificato come > 0 anche se blu. Rispostando la figura più in alto si risolve ottenendo la retta nera di valori $\\mathbf{w}_1 = -6, \\mathbf{w}_2 = 6, b = 2$.\n",
    "\n",
    "  - Questa implementazione base converge solamente se i dati sono linearmente separabili.\n",
    "    \n",
    "    - L'errore non potrà mai essere 0 qualora i dati non siano separabili, dunque il ciclo non terminerà mai.\n",
    "\n",
    "  - È il progenitore delle **reti neurali**\n",
    "    - non a caso esiste un modello **multi-layer perceptron** che è una rete neurale su cui ogni layer è un perceptron messo a cascata\n",
    "\n",
    "\n",
    "- Usando l'esempio di partenza sulle cellule tumorali \n",
    "\n",
    "  - siamo in $\\mathbb{R}^2$ (2 features)\n",
    "    \n",
    "    - $x_1 = mean_area$\n",
    "    - $x_2 = mean_concave_points$\n",
    "\n",
    "  - si vuole classificare se sono **benigne** o **maligne**\n",
    "\n",
    "    - definiamo una variabile target $y$ tale che\n",
    "    $$\n",
    "    y = \\begin{cases}\n",
    "      -1 \\text{ se la cellula è benigna} \\\\\n",
    "      +1 \\text{ se la cellula è maligna}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "  - La separazione delle classi si effettua tramite un iperpiano (che in questo caso è una retta) definito da\n",
    "    $$\n",
    "      w_1x_1 + w_2x_2 + b = 0\n",
    "    $$\n",
    "\n",
    "  - dove $w_1, w_2, b$ sono i parametri da stimare in modo tale che\n",
    "    \n",
    "    - se $w_1x_1 + w_2x_2 + b > 0$ allora $y = +1$, cioè la cellula è maligna\n",
    "\n",
    "    - se $w_1x_1 + w_2x_2 + b < 0$ allora $y = -1$, cioè la cellula è benigna\n",
    "\n",
    "  - ovvero\n",
    "    $$\n",
    "    y = \\begin{cases}\n",
    "    -1 \\quad \\text{if } \\mathbf{w \\cdot x} + b < 0 \\\\\n",
    "    +1 \\quad \\text{if } \\mathbf{w \\cdot x} + b \\geq 0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "\n",
    "  - Supponiamo che l'algoritmo abbia trovato la seguente retta \n",
    "    $$\n",
    "    0.007 \\cdot mean\\_area + 67.443 \\cdot mean\\_concave\\_points - 8.287 = 0\n",
    "    $$\n",
    "\n",
    "  - la classificazione di due nuove cellule sarà semplice\n",
    "\n",
    "    - $y^{(1)} = \\mathbf{x}^{(1)} = (mean\\_area = 500, mean\\_concave\\_points = 0.025)$ da cui otterremo \n",
    "    $$\n",
    "    \\mathbf{w \\cdot x}^{(1)} + b = 0.007 \\cdot 500 + 67.443 \\cdot 0.025 - 8.287 = -3.035\n",
    "    $$\n",
    "    dunque una cellula benigna\n",
    "\n",
    "    - $y^{(2)} = \\mathbf{x}^{(2)} = (mean\\_area = 500, mean\\_concave\\_points = 0.075)$ da cui otterremo\n",
    "    $$\n",
    "    \\mathbf{w \\cdot x}^{(2)} + b = 0.007 \\cdot 500 + 67.443 \\cdot 0.075 - 8.287 = 0.271\n",
    "    $$\n",
    "    è una cellula maligna ma molto boarder line in quanto molto vicina all'iperpiano.\n",
    "\n",
    "\n",
    "## Come trovare l'iperpiano di separazione\n",
    "- Si può riscrivere la separazione con etichette\n",
    "  $$\n",
    "      y = \\begin{cases}\n",
    "      -1 \\quad \\text{if } \\mathbf{w \\cdot x} + b < 0 \\\\\n",
    "      +1 \\quad \\text{if } \\mathbf{w \\cdot x} + b \\geq 0\n",
    "      \\end{cases}\n",
    "  $$\n",
    "  come $-y(b+ \\mathbf{w \\cdot x}) < 0$.\n",
    "  \n",
    "  - Infatti una cellula classificata correttamente (indipendentemente dalla classe di appartenenza), con questa forma compatta darà valore negativo, mentre una cellula classificata erroneamente darà valore positivo.\n",
    "\n",
    "    - ad esempio se $y=-1$ (cellula benigna), $b+\\mathbf{w \\cdot x} = -1$ (cioè la valutazione sulla base delle feature $\\mathbf{x}$ e ell'iperpiano $\\mathbf{w}$ è corretta) allora $-y(b+\\mathbf{w \\cdot x}) = 1 \\cdot -1 < 0$ (classificata correttamente)\n",
    "    - se invece $y=1$ (cellula maligna), nonostante $b+\\mathbf{w \\cdot x} = -1$ (cioè la valutazione sulla base delle feature $\\mathbf{x}$ e ell'iperpiano $\\mathbf{w}$ è errata) allora $-y(b+\\mathbf{w \\cdot x}) = -1 \\cdot -1 > 0$ (classificata erroneamente)\n",
    "\n",
    "- A questo punto minimizzando rispetto a $b$ e $\\mathbf{w}$ la somma di questa espressione sulle $m$ istanze di training, si minimizza l'errore di classificazione.\n",
    "  $$\n",
    "    \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\text{max}(0, -y_i(b + \\mathbf{w \\cdot x}_i))\n",
    "  $$\n",
    "  \n",
    "  - considerando che $-y(b+ \\mathbf{w \\cdot x}) < 0$ è equivalente a $\\text{max}(0, -y_i(b + \\mathbf{w \\cdot x}_i))$.\n",
    "\n",
    "### Come minimizzare la funzione: softmax\n",
    "- La funzione appena mostrata (visibile nella figura sottostante in verde) è però non derivabile, poiché in 0 ha uno spigolo, dunque non si può applicare la discesa del gradiente per minimizzarla.\n",
    "\n",
    "<img src=\"imgs/iperpianofunzione.PNG\" alt=\"\" width=350>\n",
    "\n",
    "- L'idea è quella di trovare una funzione che la approssimi (in figura tratteggiata) e che sia derivabile.\n",
    "  \n",
    "  - Definendo la funzione da minimizzare come $\\text{max}(0, s)$, la funzione che la approssima, chiamata **softmax** (il nome deriva dal fatto che è una versione smooth (soft) della max), è definita come segue\n",
    "  $$\n",
    "  \\text{softmax}(0,s) = \\log \\left( 1 + e^s \\right)\n",
    "  $$ \n",
    "    \n",
    "    - questa funzione è covessa, continua e derivabile\n",
    "\n",
    "  - Minimizzando la somma rispetto a $b$ e $\\mathbf{w}$ della softmax su tutte le istanze, si minimizza l'errore di classificazione.\n",
    "  $$\n",
    "    \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\log \\left( 1 + e^{-y_i(b + \\mathbf{w \\cdot x}_i)} \\right)\n",
    "  $$\n",
    "\n",
    "\n",
    "## Regressione logistica\n",
    "- Per la **regressione logistica** si minimizza la funzione data dalla somma della softmax e la **regolarizzazione** L2 dei parametri con peso $\\lambda$\n",
    "\n",
    "$$\n",
    "  \\underset{b, \\mathbf{w}}{\\text{minimize}} \\sum_{i=1}^m \\log \\left( 1 + e^{-y_i(b + \\mathbf{w \\cdot x}_i)} \\right) + \\frac{\\lambda}{2} \\vert \\vert \\mathbf{w} \\vert \\vert^2_2\n",
    "$$\n",
    "\n",
    "  - Il motivo per cui si aggiunge la regolarizzazione è che senza di essa l'errore minimo trovato può anche essere molto basso per il training set, ma molto elevato per il test set, in altre parole, minimizzare un errore non regolarizzato può portare a dell'**overfitting**.\n",
    "\n",
    "  - Ponendo questo ulteriore vincolo, imponiamo che l'errore giaccia su una circonferenza centrata sull'origine.\n",
    "\n",
    "    - Ci allontaneremo dal minimo globale, ma poiché saremo più vicini all'origine i valori di $w_1, w_2$ che sono i parametri che definiscono l'iperpiano saranno più piccoli in valore assoluto. Più piccoli sono i parametri e **meno oscilazioni ci sono nell'iperpiano**.\n",
    "\n",
    "<img src=\"imgs/regolarizzazione.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- L'iperpiano appreso facendo la discesa del gradiente ci consente di individuare una funzione che faccia da **classificatore lineare**, ossia una funzione che, dato un dato in input, stabilisca a quale classe appartiene.\n",
    "$$\n",
    "  \\sigma(h_{\\mathbf{\\tilde{w}}}(\\mathbf{\\tilde{x}})) = \\frac{1}{1 + e^{-h_{\\mathbf{\\tilde{w}}}(\\mathbf{x})}} = \\frac{1}{1 + e^{-\\mathbf{\\tilde{w}} \\cdot \\mathbf{\\tilde{x}}}} = \\frac{1}{1 + e^{-(\\mathbf{w} \\cdot \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "  - dove $h_{\\mathbf{\\tilde{w}}}(\\tilde{\\mathbf{x}}) = -y_i(b + \\mathbf{w \\cdot x}_i)$ e $b+ \\mathbf{w \\cdot x}$ è l'iperpiano individuato.\n",
    "\n",
    "- In conclusione la regressione logistica è una funzione che\n",
    "  \n",
    "  - dato il parametro in input $\\sigma(t) = \\frac{1}{1+e^{-(b+w\\cdot t)}}$ dove $t$ è l'istanza da classificare (ad esempio una nuova cellula)\n",
    "  \n",
    "  - restituisce un valore in $[0,1]$ che è la probabilità che l'istanza appartenga alla classe positiva (ad esempio maligna).\n",
    "    \n",
    "    - In altre parole la funzione è definita come $\\mathbb{R} \\rightarrow [0,1]$\n",
    " \n",
    "  - Questa funzione approssima una **funzione a gradino** \n",
    "    \n",
    "    - In particolare se $\\mathbf{x}$ giace sull'iperpiano allora $\\mathbf{w \\cdot x} + b = 0$ e dunque $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^0} = 0.5$, cioè non sappiamo se la cellula appartiene ad una classe o all'altra, poiché la probabilità che appartenga ad una rispetto all'altra è uguale (sempre 0.5).\n",
    "    - Se invece risulta $\\mathbf{w \\cdot x} + b > 0$ allora $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^{-(b + \\mathbf{w \\cdot x})> 0}} > 0.5$, cioè l'istanza appartiene alla prima classe.\n",
    "    - Se al contrario abbiamo un valore negativo, cioè $\\mathbf{w \\cdot x} + b < 0$ allora $\\sigma(\\mathbf{x}) = \\frac{1}{1+e^{-(b + \\mathbf{w \\cdot x})< 0}} > 0.5$, classe (-1).\n",
    "\n",
    "<img src=\"imgs/regrlogistica.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Questi risultati possono essere generalizzati in maniera che valgano \n",
    "  - ovviamente per istanze con più di una sola feature (da qui il vettore $\\mathbf{x}$)\n",
    "  - Per più di due classi (da qui il vettore $\\mathbf{w}$)\n",
    "\n",
    "\n",
    "## Regressione logistica multivariata lineare e non lineare\n",
    "- Abbiamo già anticipato che è possibile generalizzare la regressione logistica per classificare istanse che non sono classificabili linearmente. \n",
    " \n",
    "  - Questo significa che lo spazio in cui sono rappresentate le istanze non ha un iperpiano che le suddivida in classi distinte senza errori.\n",
    "\n",
    "- Sia $\\mathbf{x} \\in \\mathbb{R}^n$ un'istanza $\\mathbf{x} = (x_1, \\dots, x_n)$ in $n$ variabili\n",
    "  \n",
    "  - la **funzione di classificazione lineare multivariata** è\n",
    "    $$\n",
    "    \\sigma(x_1, \\dots, x_n) = \\frac{1}{1+e^{-h_w(x_1, \\dots, x_n)}}\n",
    "    $$\n",
    "    \n",
    "    dove $h_w(x_1, \\dots, x_n) = b + w_1x_1 + \\dots + w_nx_n$ è l'**iperpiano di separazione**, chiaramente di dimensione $n$ se $n$ è il numero di features (o dimensione dello spazio che deve tagliare).\n",
    "\n",
    "    - l'individuazione di questo iperpiano avviene nella stessa modalità che abbiamo visto prima: minimizzando la funzione di softmax su tutte le istanze di training.\n",
    "\n",
    "- La **regressione logistica non lineare**, ad esempio con un polinomio di grado 2 avviene nel seguente modo\n",
    "    $$\n",
    "      \\sigma(x_1, \\dots, x_n) = \\frac{1}{1+e^{h_w^2(x_1, \\dots, x_n)}} \n",
    "    $$\n",
    "\n",
    "    dove \n",
    "\n",
    "    $$h_w^2(x_1, \\dots, x_n) = \\sum_{i=1}^n w_{i,1}x_i^2 + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{i,j}x_ix_j\n",
    "\n",
    "    $$\n",
    "\n",
    "    è l'iperpiano di separazione, chiaramente di dimensione $n$ se $n$ è il numero di features (o dimensione dello spazio che deve tagliare).\n",
    "\n",
    "  - In altre parole applichiamo una trasformazione alle $n$ features in modo da renderle un polinomio di secondo grado (scritto in forma compatta come $h_w^2(x_1, \\dots, x_n)$)\n",
    "\n",
    "  - Il vantaggio nel fare questo è che passiamo da uno spazio a $n$ dimensioni (una per ogni feature) a uno spazio di \n",
    "\n",
    "    $$\n",
    "      \\binom{n+g-1}{g}\n",
    "    $$\n",
    "    \n",
    "    dimensioni, dove $g$ è il grado del polinomio (in questo caso 2).\n",
    "\n",
    "  - Questo può essere un problema se vogliamo aumentare di molto la dimensionalità del problema o in presenza di molte feature, poiché potremmo aver bisogno di una capacità computazionale elevata.\n",
    "\n",
    "  - A tal proposito vengono in aiuto le **funzioni Kernel** che consentono di aumentare la dimensionalità dello spazio senza però calcolarsi tutte le feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito mostriamo come fare una regressione logistica in python con `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=5)), #grado 5\n",
    "    ('logreg', LogisticRegression(C=0.0011))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andando ad addestrare il modello con\n",
    "```python\n",
    "  model.fit(X_train, y_train)\n",
    "```\n",
    "sarà poi possibile visionare i parametri $w$ del modello con\n",
    "```python\n",
    "  model.named_steps['logreg'].coef_.T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione Multiclasse con Iperpiani\n",
    "- Abbiamo già detto che la classificazione può essere fatta anche su più di due classi.\n",
    "\n",
    "- Per fare questo esistono due metodi per lavorare con $C>2$ classi \n",
    "    1. **One-vs-All** \n",
    "    2. **Multinominal** \n",
    "\n",
    "### One-vs-All\n",
    "- Per poterlo usare in python occorre inserire nella funzione `LogisticRegression` il parametro `multi_class='ovr'`.\n",
    "\n",
    "- Questo metodo consiste nell'individuare $C$ iperpiani, uno per ogni classe, che separano la classe in questione dalle altre.\n",
    "\n",
    "    - come mostra l'immagine con tre classi seleziono a turno un iperpiano che prima divide le istanze rosse dalle altre, poi le blu e infine le verdi\n",
    "\n",
    "        <img src=\"imgs/ovo.PNG\" alt=\"\" width=400>\n",
    "\n",
    "\n",
    "    - formalmente, pertanto, abbiamo $C$ iperpiani di separazione, uno per ogni classe $c \\in \\{1, \\dots, C\\}$, definiti come\n",
    "        $$\n",
    "            b_c + \\mathbf{x}^T\\mathbf{w}_c = 0, \\quad c = 1, \\dots, C\n",
    "        $$\n",
    "\n",
    "- Questo approccio è molto interessante poiché **parallelizzabile**\n",
    "\n",
    "    - infatti ogni iperpiano può essere individuato in maniera indipendente dagli altri\n",
    "\n",
    "- Trovati i $C$ iperpiani, occorre applicare una **regola di fusione** per capire come assegnare una classe ad una nuova istanza $\\mathbf{x}$.\n",
    "\n",
    "    - cioè ad ogni istanza $\\mathbf{x}$ dobbiamo assegnare la classe $y$ corrispondente all'iperpiano $j$ che masimizza\n",
    "\n",
    "        $$\n",
    "            y = \\underset{j = 1, \\dots, C}{\\text{argmax }} b_j + \\mathbf{x}^T\\mathbf{w}_j\n",
    "        $$\n",
    "\n",
    "        cioè arrivata una nuova istanza, proviamo ad assegnare tutti gli iperpiani trovati e scegliamo quello che massimizza la funzione sopra, poiché sarà quello che classifica meglio l'istanza.\n",
    "\n",
    "    - con `model.coef_.T` si ottengono $C$ array di parametri $\\mathbf{w}_c$ per $c=1, \\dots, C$.\n",
    "\n",
    "\n",
    "### Multinomial\n",
    "- In questo caso per usarlo in python si deve inserire nella funzione `LogisticRegression` il parametro `multi_class='multinomial'`.\n",
    "\n",
    "- Questo approccio individua *congiuntamente* i $C$ iperpiani minimizzando contemporaneamente la regola di fusione \n",
    "    $$\n",
    "        y = \\underset{j = 1, \\dots, C}{\\text{argmax }} b_j + \\mathbf{x}^T\\mathbf{w}_j\n",
    "    $$\n",
    "\n",
    "- Ogni istanza $\\mathbf{x}_p$ è classificata correttamente nella propria classe $c$ se \n",
    "    $$\n",
    "        b_c + \\mathbf{x}_p^T \\mathbf{w}_c = \\underset{j = 1, \\dots, C}{\\text{max }} \\left( b_j + \\mathbf{x}_p^T \\mathbf{w}_j \\right)\n",
    "    $$\n",
    "    esattamente come prima.\n",
    "    \n",
    "- Questa uguaglianza si può riscrivere come\n",
    "    $$\n",
    "        \\underset{j = 1, \\dots, C}{\\text{max }} \\left( b_j + \\mathbf{x}_p^T \\mathbf{w}_j \\right) - \\left( b_c + \\mathbf{x}_p^T \\mathbf{w}_c \\right) = 0\n",
    "    $$\n",
    "\n",
    "- Per quanto detto sopra nella sezione sulla softmax,  $\\underset{j = 1, \\dots, C}{\\text{max }} \\left( b_j + \\mathbf{x}_p^T \\mathbf{w}_j \\right) > 0$ solo con errori (ricordiamo che $-y(b+ \\mathbf{w \\cdot x}) < 0$ da valore > 0 se vi è un errore di classificazione), perciò la loss da minimizzare è proprio quella funzione. \n",
    "\n",
    "    - come fatto sopra si può riscrivere la loss come $\\text{max } \\sum_{j=1}^C(0, b + \\mathbf{x}_p^T \\mathbf{w}_j)$. Questa funzione non è derivabile e pertanto la sostituiamo con la **multiclass softmax** (generalizzazione della softmax).\n",
    "        $$\n",
    "            \\text{soft}(s_1, \\dots, s_C) = \\log \\left( \\sum_{j=1}^C  e^{s_j} \\right)\n",
    "        $$\n",
    "\n",
    "    - per minimizzare questa funzione deriviamo la seguente funzione\n",
    "\n",
    "        $$\n",
    "            g(b_1, \\dots, b_c, \\mathbf{w}_1, \\dots, \\mathbf{w}_c) = \\sum_{c=1}^C \\sum_{p \\in \\Omega_c} \\left[ \\underset{j = 1, \\dots, C}{\\text{max }} \\left( b_j + \\mathbf{x}_p^T \\mathbf{w}_j \\right) - \\left( b_c + \\mathbf{x}_p^T \\mathbf{w}_c \\right) \\right] = - \\sum_{c=1}^C \\sum_{p \\in \\Omega_c} \\log \\left( \\frac{e^{b_c + \\mathbf{x}_p^T \\mathbf{w}_C}}{\\sum_{j=1}^C e^{b_j + \\mathbf{x}^T_p \\mathbf{w}_j}} \\right)\n",
    "        $$\n",
    "\n",
    "## Classificazione con classi sbilanciate\n",
    "- Può capitare di avere a disposizione dataset le cui istanze appartenenti ad una classe siano molto inferiori a quelle dell'altra. \n",
    "    \n",
    "    - Vi è cioè una **suddivisione sbilanciata** fra classi.\n",
    "\n",
    "- tipicamente i problemi reali sono di questo tipo poiché magari una classe rappresenta la normalità e l'altra l'anomalia.\n",
    "\n",
    "    - ad esempoio classificando le transazioni di carte di credito in \"lecite\" e \"illecite\", le prime saranno molto più numerose delle seconde.\n",
    "\n",
    "    - analizzando le cellule tumorali, quelle benigne saranno molto più numerose di quelle maligne.\n",
    "\n",
    "\n",
    "<img src=\"imgs/classisbilanciate.PNG\" alt=\"\" width=400>\n",
    "\n",
    "\n",
    "- Il problema in questo è che gli algoritmi di classificazione, esattamente come ogni altro algoritmo di ML, avendo un dataset sbilanciato\n",
    "\n",
    "    - impara molto bene a classificare istanze appartenenti alla classe più numerosa\n",
    "\n",
    "    - impara molto male a classificare quelle della classe meno numerosa\n",
    "\n",
    "- Esistono dei rimedi al problema, che però non lo risolvono completamente\n",
    "\n",
    "    - La prima cosa che si può fare è **pesare diversamente gli errori** sulle diverse classi\n",
    "        \n",
    "        - Negli approcci visti fin'ora un errore produce lo stesso aumento nella loss indipendentemente dalla classe di appartenenza dell'istanza (ricordiamo che la loss è la funzione da minimizzare per ridurre l'errore di classificazione).\n",
    "\n",
    "        - Introdurre un peso differenziato significa riadattare la loss in modo tale che un errore su una classe meno rappresentata sia più penalizzante di un errore su un'altra classe con più istanze.\n",
    "\n",
    "        - Questo si ottiene con il parametro `class_weight` da fornire in fase di inizializzazione del modello\n",
    "\n",
    "            ```python\n",
    "                class_weight = {1: 10}\n",
    "            ```\n",
    "\n",
    "            questo significa che do peso 10 volte superiore all'errore di classificazione di una classe rispetto all'altra. Se fossimo in un sistema multiclasse avremmo\n",
    "\n",
    "            ```python\n",
    "                class_weight = {1: 10, 2: 1, 3: 1}\n",
    "            ```\n",
    "            dove la prima classe ha peso 10 volte superiore alle altre due poiché quella con meno istanze, mentre le altre due hanno lo stesso peso.\n",
    "    \n",
    "    - Un'altra tecnica è l'**undersampling** della classe più numerosa. \n",
    "        \n",
    "        - Si vanno cioè a scegliere casualmente un sottoinsieme di istanze della classe più numerosa in modo tale che il numero di istanze sia pari a quello della classe meno numerosa.\n",
    "\n",
    "        - Poiché a seconda dei dati scelti l'iperpiano trovato può variare poiché magari non si prendono in considerazione istanze boerderline. Per questo motivo si ripete l'operazione più volte e si sceglie l'iperpiano che ha la loss più bassa.\n",
    "\n",
    "    - Si può anche fare **oversampling** deòòa classe meno numerosa.\n",
    "\n",
    "        - Si prende cioè la classe meno rappresentata e, attraverso tecniche di interpolazione, si generano nuove istanze artificiali di questa classe. Questo è anche detto **data augmentation**.\n",
    "\n",
    "        - Una di queste tecniche si chiama **SMOTE**\n",
    "\n",
    "            ```python\n",
    "                from imblearn.over_sampling import SMOTE   \n",
    "            ```\n",
    "\n",
    "        - la cosa importante nel fare ciò è che si mettano in test set solo istanze che non sono state generate artificialmente, poiché altrimenti si avrebbe un test set che non è rappresentativo della realtà. Il rischio è quello che il modello impari a classificare bene solo le istanze generate artificialmente, ma non quelle reali.\n",
    "\n",
    "## Unità neuronali\n",
    "- È interessante notare che la regressione logistica rappresenta un'unità neurale (un perceptron).\n",
    "\n",
    "- Abbiamo detto che la **funzione di classificazione** è\n",
    "\n",
    "    $$\n",
    "        \\sigma(\\mathbf{x}) = \\frac{1}{1+e^{-(b + \\mathbf{w \\cdot x})}} = \\frac{1}{1+e^{h_{\\mathbf{w}}(\\mathbf{x})}}\n",
    "    $$\n",
    "\n",
    "    dove l'espressione $h_{\\mathbf{w}}(\\mathbf{x})$ può essere riscritta come $a$, cioè\n",
    "    \n",
    "    $$\n",
    "        h_{\\mathbf{w}}(\\mathbf{x}) = a = b + \\mathbf{w \\cdot x} = b + w_1x_1 + \\dots w_nx_n\n",
    "    $$\n",
    "        \n",
    "        è l'iperpiano di separazione.\n",
    "\n",
    "- Allora possiamo definire la funzione di classificazione come $f(a) = \\frac{1}{1+e^{-a}}$ e questo ci consente di definire esattamente cos'è un **neurone**.\n",
    "\n",
    "    - Un neurone è a tutti gli effetti un classificatore \n",
    "        \n",
    "        - abbiamo in input $N$ features (o variabili)\n",
    "\n",
    "        - Queste feature vengono moltiplicati per i pesi $w_1, \\dots, w_N$ e i prodotti sommati fra loro (esattamente $w_1x_1 + \\dots + w_Nx_N$)\n",
    "\n",
    "        - A questo si aggiunge un bias $b$\n",
    "\n",
    "        - Questa quantità viene inviata ad una **funzione di attivazione** $f$ che deve essere non lineare. Una delle prime funzioni di attivazione mai usate è stata la **sigmoid** (la funzione di classificazione della regressione logistica $f(a)$ appena vista) \n",
    "\n",
    "        - Se il valore prodotto dalla funzione di attivazione, che nel caso della sigmoide restituisce un valore in $[0,1]$, supera una certa **soglia di attivazione** (iperparametro della rete neurale), il neurone si attiva e propaga il segnale al neurone successivo, altrimenti no.\n",
    "\n",
    "    - Nelle reti neurali più semplici i neuroni sono strutturati in layer. Neuroni appartenenti allo stesso layter non comunicano fra loro, possono solo comunicare con i neuroni del layer successivo.\n",
    "\n",
    "<img src=\"imgs/neuronesigmoide.PNG\" alt=\"\" width=600>\n",
    "\n",
    "## Support Vector Machines SVM\n",
    "- Abbiamo detto che è possibile avere infiniti iperpiani possibili.\n",
    "\n",
    "- Come stabiliamo qual'è il migliore?\n",
    "\n",
    "- Possiamo pensare di non ragionare più con gli iperpiani ma con i **margini**.\n",
    "\n",
    "    <img src=\"imgs/margini.PNG\" alt=\"\" width=400>\n",
    "\n",
    "- Nello specifico, come mostra l'immagine successiva, ci andiamo a calcolare, non solo l'iperpiano che divide, ma anche degli iperpiani paralleli che vadano a intercettare i punti delle due classi più vicini all'iperpiano di separazione.\n",
    "\n",
    "    - Questi punti sono detti **support vector**, cioè punti difficili.\n",
    "\n",
    "    - L'idea è quella di prendere come iperpiano, quello che ha i margini più ampi, cioè quello che ha i support vector più lontani dall'iperpiano di separazione. \n",
    "        \n",
    "        - Se si massimizza la distanza dei punti difficili (support vector) dall'iperpiano di spearazione, allora è altamente probabile che si massimizzi anche la distanza di tutti gli altri punti dall'iperpiano di separazione.\n",
    "\n",
    "- Ricercare i **support vector** è un problema di ottimizzazione quadratica.\n",
    "\n",
    "<img src=\"imgs/iperpianiparalleli.PNG\" alt=\"\" width=300>\n",
    "\n",
    "- Questo è ciò che fanno le **Support Vector Machines (SVM)**.\n",
    "\n",
    "    - Hanno come obbiettivo quello di **individuare la spearazione lineare ottimale** (secondo un criterio gemoetrico).\n",
    "\n",
    "    - Ottimi per domini ad elevata dimensionalità\n",
    "\n",
    "    - Prima dell'avvento dei transformers ritenuto l'approccio migliore per il testo\n",
    "\n",
    "    - Molto efficace con training set molto piccoli\n",
    "\n",
    "- Come sempre sia $D = \\{ (\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_r, y_m) \\}$ l'insieme delle $r$ istanze di training.\n",
    "    \n",
    "    - dove $\\mathbf{x}_i = (x_1, \\dots, x_n)$ è l'insieme di feature della medesima istanza $i$ nello spazio \\mathbb{R}^n\n",
    "\n",
    "    - y_i è la classe di appartenenza dell'istanza $\\mathbf{x}_i$, cioè $y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "- Occorre i parametri $\\mathbf{w}, b$ t.c. l'iperpiano $\\mathbf{w} \\cdot \\mathbf{x} + b = 0$ massimizzi la separazione fra i support vector delle 2 classi.\n",
    "\n",
    "- Il classificatore sarà pertanto una funzione lineare\n",
    "\n",
    "    $$\n",
    "        f(\\mathbf{x}_i) = y_i \n",
    "    $$\n",
    "\n",
    "    con \n",
    "\n",
    "$$\n",
    "    y_i = \\begin{cases}\n",
    "        +1 \\quad \\text{se } \\mathbf{w} \\cdot \\mathbf{x}_i + b > 0 \\\\\n",
    "        -1 \\quad \\text{se } \\mathbf{w} \\cdot \\mathbf{x}_i + b < 0\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
