{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "- I calcolatori riescono a risolvere problemi che non sono facilmente risolvibili con algoritmi tradizionali\n",
    "    - Riordinare alfabeticamente una lista di utenti\n",
    "    - Calcolare la media annuale di incassi per ogni categoria di prodotto\n",
    "    - ...\n",
    "- Tuttavia fanno molta difficoltà con task molto semplici per un umano\n",
    "    - Riconoscere una recensione positiva o negativa\n",
    "    - Interpretare testo e discorso\n",
    "- In generale è veramente difficile per i calcolatori riconoscere pattern di alto livello in dati non strutturati (testo, foto, audio, ecc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitazioni del Machine Learning tradizionale\n",
    "- Alla base di un buon modello di ML vi è sempre un **training set appropriato**.\n",
    "    - Centinaia di migliaia di esempi sono richiesti\n",
    "    - Nel learning supervisionato occorre aggiungere delle label agli esempi manualmente. I dati sono stati prodotti durante gli anni ma prima dell'avvento del machine learning nessuno avrebbe pensato di aggiungere delle label.\n",
    "- I modelli di ML hanno bisogno di una **rappresentazione dei dati** appropriata.\n",
    "    - La maggior parte dei modelli di ML hanno bisogno di una rappresentazione dei dati sottoforma di vettori di numeri reali.\n",
    "    - Le feature (ogni elemento del vettore) devono essere significative per il task che si vuole risolvere, cioè devono essere scelte correttamente.\n",
    "- Un modello addestrato è valido solo per il task per cui è stato addestrato, non vi è modo di fare **generalizzazione** facilmente. \n",
    "    - Un modello addestrato a classificare immagini di gatti non può essere usato per classificare immagini di cani."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "- Le reti neurali sono un tipo di modello di ML che cerca di risolvere i problemi sopra elencati.\n",
    "- Sono ispirate al funzionamento del **cervello umano**.\n",
    "    - Sono composte da **neuroni** artificiali che sono collegati tra loro in grandissime reti.\n",
    "    - Ogni neurone è attivato da quelli a cui è collegato quando la loro attivazione supera una certa soglia misurata in peso e thresholds.\n",
    "- L'idea alla base di una NN è quella di computare una funzione $\\mathbf{y} = f(\\mathbf{x})$ dove $\\mathbf{x}$ è un vettore di input e $\\mathbf{y}$ è un vettore di output.\n",
    "- Può essere usato per risolvere problemi tradizionali come la classificazione e predizione.\n",
    "    - Nel caso della classificazione, su $N$ classi di output, il modello restituisce un vettore di dimensione $N$ con la probabilità che l'istanza appartenga a ciascuna delle $N$ classi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodi\n",
    "- Come anticipato le reti neurali sono composte da **neuroni** o **nodi**\n",
    "- In ogni momento un neurone $n$ emette un valore $y_n = f_n(\\mathbf{x})$ con $\\mathbf{x} = (x_1, \\dots, x_m)$ calcolato sui valori di $x$ forniti in input dai neuroni a cui è collegato.  \n",
    "Quanto detto è riassunto nella seguente figura\n",
    "\n",
    "<img src=\"imgs/neuron.PNG\" alt=\"neurone\" width=400>\n",
    "\n",
    "- Ogni input $x_i$ è ricevuto esattamente da un nodo (l'$i$-esimo) e ogni nodo può spedire il proprio $y_n$ a più neuroni a cui è collegato.\n",
    "- Questa operazione apparentemente semplice di calcolo di $y_n$ consente di comprendere relazioni molto profonde fra gli input.\n",
    "\n",
    "### Layers\n",
    "- I nodi sono tipicamente arrangiati in **layers** (strati) e ogni layer è composto da un certo numero di nodi. L'insieme dei layer costituisce una rete neurale.\n",
    "    - Inizialmente i layer erano tipicamente tre:\n",
    "        - **Input layer**: riceve gli input e li passa al layer successivo\n",
    "        - **Hidden layer**: riceve gli input dal layer precedente e li passa al layer successivo\n",
    "        - **Output layer**: riceve gli input dal layer precedente e restituisce l'output della rete neurale\n",
    "\n",
    "\n",
    "        <img src=\"imgs/layers.PNG\" alt=\"layers\" width=500>\n",
    "    \n",
    "    - Ad oggi si è capito che con delle **reti neurali profonde** cioè composte da decine e decine di layer si ottengono risultati migliori.\n",
    "- Gli input del layer $l$ sono tutti gli output degli $l-1$ layer precedenti.\n",
    "    - Il primo layer è quello che riceve dati grezzi (non necessariamente devono seguire qualche pattern o essere strutturati).\n",
    "    - Poi vengono posti degli hidden layer in mezzo che eseguono delle computazioni su quei dati per trovare degli eventuali pattern.\n",
    "    - Infine è posto un layer di output che restituisce il risultato della computazione. \n",
    "        - Questo può cambiare in base al task che si vuole risolvere. Ad esempio nel caso della classificazione questo layer avrà tanti nodi quante sono le classi in cui si vuole classificare l'input e restituirà la probabilità che l'input appartenga a ciascuna delle classi.\n",
    "\n",
    "- L'architettura appena vista è detta **multi-layer perceptor** (MLP) ed è la più semplice architettura di rete neurale.\n",
    "\n",
    "### Funzione dei nodi\n",
    "- Come già detto i nodi eseguono una computazione $f$ sugli input che ricevono. \n",
    "- Questa funzione non è altro che una somma pesata fra il prodotto degli input e dei pesi e un threshold. \n",
    "- In particolare dato un vettore di input $\\mathbf{x} \\in \\mathbb{R}^d$, sia l'output scalare $y_j$ del nodo $j$\n",
    "$$\n",
    "    y_i = \\sigma_j \\left( \\sum_{i=1}^d w_{j,i} \\cdot x_i + b_j \\right)\n",
    "$$\n",
    "\n",
    "- il peso che ha l'arco che collega il nodo $i$ al nodo $j$, chiamato $w_{j,i}$ \n",
    "- lo moltiplichiamo per il valore di input $x_i$ corrispondente \n",
    "- la somma di tutti i prodotti pesati degli input è sommata a un valore $b_j$ chiamato **bias**. Questo bias consente ai neuroni di apprendere la tendenza generale dei dati e di adattarsi a diverse situazioni. Aggiungendo un bias, la rete neurale ha la capacità di imparare relazioni non lineari tra gli input e l'output desiderato.\n",
    "- Infine il risultato della somma è passato ad una funzione $\\sigma_j$ chiamata **funzione di attivazione** che restituisce l'output del neurone $j$.\n",
    "- Questa notazione scalare può essere sintetizzata nella seguente \n",
    "$$\n",
    "    y_i = \\sigma_j \\left( \\mathbf{W} \\cdot \\mathbf{x} + b_j \\right)\n",
    "$$\n",
    "dove $\\mathbf{W}$ è il vettore dei pesi e $\\mathbf{x}$ è il vettore degli input.\n",
    "\n",
    " - Le reti neurali dunque vanno progressivamente a migliorarsi andando a calibrare meglio i pesi della matrice $\\mathbf{W}$ e il bias $b_j$.\n",
    "\n",
    " #### Funzioni di attivazione\n",
    " - Esistono diverse funzioni di attivazione per i neuroni\n",
    "    - tipicamente è più sensato usare funzioni non lineari poiché consentono di apprendere relazioni più complicate fra i dati.\n",
    "- Le funzioni sono\n",
    "    - **Step function** usata nelle prime reti neurali, ma essendo lineare ormai non più usata. Nella sua versione più semplice funziona come segue\n",
    "    $$\n",
    "        \\sigma(x) = \\begin{cases}\n",
    "            1 & \\text{se } x \\geq \\text{soglia} \\\\\n",
    "            0 & \\text{altrimenti}\n",
    "        \\end{cases}\n",
    "    $$\n",
    "    - **Sigmoid function** ad oggi è forse una delle più utilizzate. \n",
    "        - è una funzione non lineare che ha un output compreso fra 0 e 1. \n",
    "        - è una versione continua e più soft della step function\n",
    "        - la sua formulazione è la seguente \n",
    "        \n",
    "    $$\n",
    "        \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "    $$\n",
    "\n",
    "\n",
    "## Come addestrare le reti neurali\n",
    "- Occorre come prima cosa avere dei dati etichettati della forma $(\\mathbf{x}_i, \\mathbf{g}_i)$ dove\n",
    "    - $\\mathbf{x}_i$ è un vettore di input fornito alla rete\n",
    "    - $\\mathbf{g}_i$ è un vettore di output che ci si aspetta venga restituito dalla rete neurale (per guidarla a capire cosa deve fare)\n",
    "- Per ogni input $\\mathbf{x}_i$ la rete neurale restituisce un vettore $\\mathbf{y}_i$ che è la sua predizione.\n",
    "- L'obbiettivo della rete è quello di **minimizzare la differenza* fra $\\mathbf{y}_i$ e $\\mathbf{g}_i$ per ogni $i$.\n",
    "- Per fare ciò si usa una funzione di costo chiamata **loss** $C(\\mathbf{y}_i, \\mathbf{g}_i)$ che misura la differenza fra i due vettori.\n",
    "    - Si calcola su ogni **batch** di training. Un batch è un sottoinsieme dei dati di training che viene usato per addestrare la rete. \n",
    "    - Esistono diverse funzioni di loss che servono a misurare l'accuratezza del modello\n",
    "    - La più semplice è l'**errore quadratico medio** (MSE)\n",
    "    $$\n",
    "        \\text{MSE} = \\frac{1}{2N} \\sum_{i=1}^N \\vert \\vert \\mathbf{y}_i - \\mathbf{g}_i \\vert \\vert^2\n",
    "    $$\n",
    "    - Si può tenere traccia della loss ad ogni iterazione di training e plottarla per vedere se il modello sta migliorando o meno.\n",
    "        <img src=\"imgs/loss.PNG\" alt=\"loss-epoch\" width=300>\n",
    "    - La modalità con cui si minimizza l'errore è chiamata **back-propagation** o discesa del gradiente. È lo stesso approccio usato nel caso della regressione: si calcola una funzione d'errore e si cerca di minimizzarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiti delle reti feed forward\n",
    "Le reti trattate fino ad ora vengono definite **feed forward** poiché l'informazione fluisce in una sola direzione, da sinistra verso destra. Questo tipo di reti hanno dei limiti:\n",
    "- Non consentono di mantere memoria di ciò che è stato visto in precedenza (lo stato).\n",
    "    - Ad esempio se si vuole comprendere il significato di un testo l'ordine delle parole è importante. Con le reti feed-forward questo non si riesce a fare\n",
    "\n",
    "## Reti neurali ricorrenti (RNN)\n",
    "Queste reti introducono il concetto di **ciclo**. Ogni layer non solo può inviare il proprio output ai layer successivi ma anche a se stesso. In questo modo si riesce a mantenere lo stato e a comprendere l'ordine delle cose.\n",
    "- In altre parole si possono avere cicli \n",
    "- I cicli consentono di processare informazioni ottenute in momenti differenti\n",
    "- Le informazioni processate in un ciclo costituiscono uno **stato interno** della rete\n",
    "- L'output di un nodo è funzione sia dll'input che dello stato interno\n",
    "- Tutto questo consente di effettuare **processazioni context-dependent**\n",
    "    - ad esempio predire la prossima parola in un testo richiede la conoscenza di tutte le parole precedenti.\n",
    "\n",
    "In poche parole lo schema è il seguente \n",
    "\n",
    "<img src=\"imgs/rnn.PNG\" alt=\"rnn\" width=180>\n",
    "\n",
    "dove nell'hidden layer è presente un ciclo con un **neurone di contesto** che mantiene lo stato.\n",
    "\n",
    "- Le **context unit** propagano lo stato attraverso un arco di peso $\\mathbf{U}$ all'hidden layer (insieme all'input) e questo viene usato per calcolare l'output del neurone all'iterazione corrente. \n",
    "    - Se definiamo la sequenzialità come la sequenza di $t_0 \\rightarrow t_1 \\rightarrow \\dots \\rightarrow t_n$ allora l'output del neurone $j$ all'iterazione $t$ è dato da\n",
    "    $$\n",
    "    \\mathbf{h}_t = \\sigma_t \\left( \\mathbf{W} \\cdot \\mathbf{x}_t + \\mathbf{U} \\cdot \\mathbf{t}_{t-1} + \\mathbf{b} \\right)\n",
    "    $$\n",
    "    \n",
    "## Attention\n",
    "\n",
    "Ad un certo punto ci si è resi conto che le RNN funzionavano molto bene per task di traduzione automatica del testo basati su un'architettura **encoder-decoder** e reti neurali ricorrenti che funzionava come segue\n",
    "- Una rete chiamata **encoder** prendeva in input una frase in una lingua e la codificava in un vettore di dimensione fissa\n",
    "- Una rete chiamata **decoder** prendeva in input l'output dell'encoder e restituiva una sua rappresentazione in lingua inglese\n",
    "\n",
    "Tuttavia questa architettura faceva difficoltà a tenere in memoria frasi molto lunghe. Questo portava poi il decoder a fare una conversione solo di una parte del testo, sbagliando il risultato finale. Su questo è nato il meccanismo di **attention** che semplicemente consiste nel dare un peso maggiore alle parole più importanti di una frase. In questo modo il decoder può concentrarsi su quelle parole e ignorare le altre.\n",
    "- L'idea pertanto è quella di fornire in input al decoder la somma pesata di tutti gli output dell'encoder ad ogni iterazione.\n",
    "- I pesi assegnati alla sequenza in input vengono imparati durante l'addestramento da una rete neurale chiamata **attention layer** (o **alignment model**).\n",
    "\n",
    "\n",
    "<img src=\"imgs/attention.PNG\" alt=\"rnn\" width=700>\n",
    "\n",
    "In questa immagine vediamo \n",
    "- l'encoder che non è altro che una rete neurale ricorrente. In particolare infatti per ogni input fornito in sequenza temporale (prima $\\mathbf{x}_0$ poi $\\mathbf{x}_1$ e cosi via) i vari neuroni comunicano avanti e indietro (frecce arancioni e viola) per fornire un output.\n",
    "- dall'altro lato il decoder, anch'esso RNN, all'iterazione 3 (vediamo infatti che sta calcolando $\\mathbf{y}'_3$) prende in input tutti gli ouput dell'encoder, ne fa una somma pesata (i cui pesi vengono presi dall'allignment model (attention layer) a destra) e calcola l'output.\n",
    "\n",
    "Supponendo di avere tante iterazioni del decoder quanti sono gli input dell'encoder (cioè la lunghezza dell'input $L$), allora dovremo calcolare circa $L^2$ pesi, poiché per ogni output del decoder dovremo calcolare la somma pesata di tutti gli output dell'encoder.\n",
    "\n",
    "- Come si può vedere l'attention layer è una rete neurale feed forward che prende in input gli output dell'encoder e restituisce i pesi che verranno usati per calcolare la somma pesata. Abbiamo un neurone per ogni output dell'encoder e la modalità di calcolo dei pesi è la seguente (si basa s una funzione di **softmax**) che si calcola cosi\n",
    "$$\n",
    "\\alpha_{(t,i)} = \\frac{exp(e_{(t,i)})}{\\sum_{j} exp(e_{(t,j)})}\n",
    "$$\n",
    "\n",
    "Esistono molti tipi di attention layer, fra cui\n",
    "- **Concatenative Attention** calcolato come segue\n",
    "    $$\n",
    "    e_{(t,i)} = \\mathbf{v}^T \\tanh \\left( \\mathbf{W} \\left[ \\mathbf{h}_{(t)} ; \\mathbf{y}(i) \\right] \\right)\n",
    "    $$\n",
    "    dove $\\mathbf{v}$ è un parametro di scaling e ; è l'operatore di concatenazione.\n",
    "\n",
    "\n",
    "- **Multiplicative Attention** che nasce per accellerare i calcoli della concatenative attention. Non usa più la tangente ed è ottimizzata per le matrici.\n",
    "    $$\n",
    "    e_{(t,i)} = \\mathbf{h}_{(t)}^T \\mathbf{W} \\mathbf{y}(i)\n",
    "    $$\n",
    "\n",
    "- la più semplice è quella **dot-product** che calcola i pesi semplificando il prodotto matriciale per $\\mathbf{W}$ come nel caso di multiplicative attention. Si calcola come segue\n",
    "$$\n",
    "e_{(t,i)} = \\mathbf{h}_{(t)}^T \\mathbf{y}(i)\n",
    "$$\n",
    "\n",
    "\n",
    "## Attention is all you need: Transformer\n",
    "- Nel 2017 ci si è resi conto che le reti neurali ricorrenti non erano necessarie per il meccanismo di Attention. \n",
    "    - I layer ricorrenti generano dei problemi essendo poco ottimizzati per il calcolo parallelo. Infatti per ogni input devo aspettare che tutti gli input precedenti siano stati processati.\n",
    "- Nasce cosi il modello **Transformer** schematizzato nella seguente immagine\n",
    "\n",
    "<img src=\"imgs/transformer.png\" alt=\"\" width=400>\n",
    "\n",
    "- L'idea è quella di togliere le reti neurali ricorrenti ma mantenere l'attention\n",
    "- Questo è ottenuto aggiungendo un layer chiamato **Multi-Head Attention** che non fa altro che calcolare l'attention dot product senza usare reti ricorrenti.\n",
    "- A sinistra abbiamo un encoder e a destra un decoder.\n",
    "- Possiamo avere più blocchi di questo tipo concatenati (da qui la notazione in figura Nx), in modo da avere una rete più profonda.\n",
    "\n",
    "### Input di un transformer \n",
    "- Data una frase di $L$ parole scomposta nei rispettivi $L$ token, allora a ciascun token dovrò associare un **embedding**. Questo embedding è un vettore di dimensione $d$ che rappresenta il token.\n",
    "    - Per ottenere un embedding si usa un layer chiamato **embedding layer** (in figura **input embedding**) che non è altro che un layer prende in input un token e restituisce il suo embedding. \n",
    "    - Questo layer combina tutti gli embedding in un'unica matrice $\\mathbf{Z}$ di dimensione $L \\times d$ dove $L$ è la lunghezza della frase.\n",
    "        - Poiché $L$ è costante, se l'input dovesse essere più corto si aggiungono dei valori di padding\n",
    "    - Il problema a questo punto è che la rete, non essendo più ricorrente, perde il concetto di *sequenzialità* ovvero non si riesce più a capire se un input viene prima o dopo di un'altro.\n",
    "        - Per questa ragione è stato aggiunto un altro layer chiamato **positional embedding** che dice, per ogni token in input, qual'è la sua posizione nella frase.\n",
    "        - Per fare questo il positional embedding calcola una nuova matrice $\\mathbf{P}$ contenente questi positional embedding.\n",
    "        - La matrice $\\mathbf{P}$ può essere dedotta dalla retre stessa o calcolata a priori con la seguente formula: detta $i$ la posizione della parola nella frase e $j$ l'indice del word embedding, allora\n",
    "        $$\n",
    "            p_{i,j} = \\begin{cases}\n",
    "                \\sin \\left( \\frac{1}{10000 \\frac{j}{d}} \\right) \\quad \\text{se } j \\text{ è pari} \\\\\n",
    "                \\cos \\left( \\frac{1}{10000 \\frac{j-1}{d}} \\right) \\quad \\text{se } j \\text{ è dispari}\n",
    "            \\end{cases}\n",
    "        $$\n",
    "\n",
    "- A questo punto, dunque, l'input di un **multi-head attention** è la matrice $ \\mathbf{X} \\in \\mathbb{R}^{L \\times d} = \\mathbf{Z} + \\mathbf{P}$\n",
    "    - Questo layer calcola la Scaled Dot Product $h$ volte utilizzando pesi differenti, cioè proietto gli embeddings $h$ volte in maniera diversa ottenendo $h$ rappresentazioni diverse dell'input. Cioè la rete impara delle relazioni semantiche diverse in ognuno dei layer.\n",
    "    - Una volta calcolati questi $h$ risultati vengono riconcatenati e, attraverso un layer lineare, riportati alla dimensione originale $L \\times d$.\n",
    "    \n",
    "    <img src=\"imgs/multiheadatt.png\" alt=\"\" width=300>\n",
    "\n",
    "    - Per calcolare la Scaled Dot-Product Attention, è necessario avere tre matrici ottenute applicando le seguenti trasformazioni a $\\mathbf{X}$\n",
    "    1. key $\\mathbf{K} \\in \\mathbb{R}^{L \\times d_k} = \\mathbf{XW}_i^K, \\quad \\mathbf{W}_i^K \\in \\mathbb{R}^{d\\times d_k}$\n",
    "    2. query $\\mathbf{Q} \\in \\mathbb{R}^{L \\times d_k} = \\mathbf{XW}_i^Q, \\quad \\mathbf{W}_i^Q \\in \\mathbb{R}^{d\\times d_k}$\n",
    "    3. value $\\mathbf{V} \\in \\mathbb{R}^{L \\times d_v} = \\mathbf{XW}_i^V, \\quad \\mathbf{W}_i^V \\in \\mathbb{R}^{d\\times d_v}$\n",
    "    \n",
    "    Il pedice $i$ indica che questi calcoli vengono fatti per ogni layer $h$ di multi-head attention, $i = 0,1, \\dots, h-1$. Le dimensioni $d_k = d_v = \\frac{d}{h}$ sono scelte in modo che il prodotto matriciale sia possibile.\n",
    "\n",
    "    - Il calcolo della Scaled Dot-Product Attention è il seguente\n",
    "    $$\n",
    "    \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left( \\frac{\\mathbf{QK}^T}{\\sqrt{d_k}} \\right) \\mathbf{V}\n",
    "    $$\n",
    "    Questo calcola la relazione che c'è fra ogni token della frase con ogni altro. Ad esempio dicendo \"il gatto è nero e il cane abbaia\" il modello capisce che \"nero\" è un aggettivo che si riferisce al gatto e non al cane.\n",
    "\n",
    "\n",
    "    - Il costo computazionale per fare questa cosa è elevato poiché per ogni token devo calcolare la relazione con tutti gli altri. In particolare abbiamo \n",
    "        - in termini di tempo $O(L^2d)$\n",
    "        - in termini di spazio $O(L^2 + Ld)$\n",
    "\n",
    "\n",
    "## Bert Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "Dalla pubblicazione del modello **Transformer** nel 2017 si è capito quando fossero promettenti queste architetture. Il primo modello di successo effettivamente implementato, utilizzante questa architettura, è BERT.\n",
    "\n",
    "- BERT è un transformer addestrato appositamente per il NLP.\n",
    "- Gli autori introducono una fase di pre-addestramento self-supervised (senza bisogno di dati etichettati in modo da poter usare dataset enormi: corpus di miliardi di testi) su due task\n",
    "    1. **Masked Language Modelling**: dato un testo in input si mascherano (nascondono) alcune parole (circa il 15% delle parole) con un token speciale [MASK], si chiede al modello di ricostruire le parole mancanti. \n",
    "        - Per valutare l'accuratezza del modello si usa il **cross entropy loss**. Detto $M$ il numero di token mascherati, $t$ i vettori one hot encoded relativi alle parole corrette e $p$ la probabilita che il modello assegni alla parola corretta, allora la loss è data da\n",
    "        $$\n",
    "        L_{MLM} = -\\frac{1}{M} \\sum_{i=1}^M \\mathbf{t}_i \\log \\mathbf{p}_i\n",
    "        $$\n",
    "    2. **Next Sentence Prediction**: date due frasi in input, il modello deve capire se la seconda frase è la continuazione della prima. In questo caso l'errore è il **binary cross entropy loss**. Detta $y \\in (0,1)$ l'indicatore della classe reale e $p$ il valore predetto\n",
    "    $$\n",
    "    L_{NSP} = -y (\\log p + (1-y) \\log (1-p))\n",
    "    $$\n",
    "\n",
    "### BERT input\n",
    "- Rispetto all'input dei transformer cosi intesi come nel paper originale, BERT ha un input diverso. \n",
    "- dato un testo in input, il pre processing di questo testo per fornirlo in input al transformer è composto da tre embeddings\n",
    "    \n",
    "    - **Token embedding**: è l'embedding di ogni token della frase. Questo embedding è ottenuto da un embedding layer che prende in input il token e restituisce il suo embedding.\n",
    "    - **Sentence embedding**: poiché posso introdurre un testo intero, questo layer restituisce degli embeddings che indicano a che frase fa riferimento ogni token\n",
    "    - **Positional embedding**: come nel caso dei transformer, questo layer restituisce degli embeddings che indicano la posizione di ogni token nel testo.\n",
    "\n",
    "    \n",
    "    <img src=\"imgs/bertinput.PNG\" alt=\"\" width=500>\n",
    "\n",
    "### BERT Training\n",
    "- Hanno addestrato BERT su 800M di parole da BookCorpus e 2500M da Wikipedia.\n",
    "- Per la fase di tokenizzazione ed embedding è stato utilizzato WordPiece.\n",
    "- Le frasi avevano lunghezza $L$ di 512 token.\n",
    "- Sono state implementate due versioni di BERT a seconda dei parametri usati\n",
    "    - $\\text{BERT}_\\text{BASE}$: 12 layer, 768 hidden units, 12 attention heads, 110M parametri\n",
    "    - $\\text{BERT}_\\text{LARGE}$: 24 layer, 1024 hidden units, 16 attention heads, 340M parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning ed Information Retrieval\n",
    "- Il **metric learning** è un sottoinsieme del machine learning che si occupa di apprendere una funzione di distanza fra due oggetti.\n",
    "    - In altre parole se ho un insieme di dati, potessi trasformarli in vettori e inserirli in uno spazio vettoriale detto **latente** (cioè non osservabile) tale per cui \n",
    "        - oggetti semanticamente simili fra loro sono vicini fra loro\n",
    "        - dati dissimili vengono messi distanti\n",
    "    - Se si riesce a fare questo altri problemi di ML diventano molto più semplici\n",
    "        - la **classificazione** diventa un problema semplicissimo di nearest neighboor.\n",
    "        - l'**information retrieval** si semplifica. Se voglio tutti i dati rilevanti per una determinata query mi sarà sufficiente calcolare la distanza fra la query e tutti i dati e restituire i più vicini.\n",
    "- Esistono due tipologie di addestramenti, sulla base della nozione di **similarità semantica** (tipicamente etichette che classificano i dati in classi) fra dati\n",
    "    - **Supervised Metric Learning**: Ogni dato è etichettato e appartiene ad una specifica classe. L'obbiettivo è quello di apprendere una funzione di distanza che avvicini dati della stessa classe e allontani dati di classi diverse.\n",
    "    - **Weakly Supervised Metric Learning**: Non ho esattamente le etichette ma so che alcuni dati sono simili fra loro e altri no. Non so perché ma ho questa informazione. L'obbiettivo anche in questo caso rimane il medesimo\n",
    "\n",
    "## Come costruire questo spazio latente\n",
    "Occorre trovare un modello matematico che ci consenta di mappare i dati in uno spazio latente.\n",
    "Per fare questo si usano strumenti come distanza di Mahalanobis.\n",
    "\n",
    "### Distanza di Mahalanobis\n",
    "- In uno spazio $\\mathbb{R}^n$ per calcolare la vicinanza fra due vettori si usa la distanza euclidea.\n",
    "- Un'alternativa è la **distanza di Mahalanobis** che è una generalizzazione della distanza euclidea.\n",
    "    - Si vuole trovare una matrice di Mahalanobis $\\mathbf{M}$ tale che, presi due elementi simili $a$ (ancora, cioè valore di riferimento), $p$ (positivo, cioè un elemento simile all'ancora) e uno dissimile $n$ (negativo), si abbia $d_M(a,p) < d_M(a,n)$ con $d_M$ distanza di Mahalanobis definita cosi\n",
    "    $$\n",
    "    d_M(x,y) = \\sqrt{(x-y)^T \\mathbf{M} (x-y)}\n",
    "    $$\n",
    "    - Ogni matrice di Mhalaobis può anche essere scritta nella forma $M = W^TW$ (poiché $M$ è semidefinita positiva), allora si ha che\n",
    "    $$\n",
    "    d_M(x,y) = \\sqrt{(x-y)^T W^TW(x-y)} = \\sqrt{(Wx - Wy)^T(Wx - Wy)} = \\vert \\vert Wx - Wy \\vert \\vert_2\n",
    "    $$\n",
    "    - Nel **deep metric learning** si usano delle reti neruali profonde (come Transformers) con pesi addestrabili $\\theta$ che mi determinino una trasfmorazione $W_\\theta$. \n",
    "\n",
    "Vediamo ad esempio che \n",
    "- nella figura (a) abbiamo lo spazio originale con tre tipologie di figure geometriche. \n",
    "- Nella figura (b) capiamo che la distanza euclidea (caso specifico di Mahalanobis) calcola la distanza fra due figure.\n",
    "    - Il nostro obbiettivo è quello di \"forzare\" la distanza euclidea ad allontanare le figure dissimili e avvicinare quelle simili (punto (c)).\n",
    "    - Per fare questo possiamo fornire in input a delle reti neurali profonde le nostre figure che calcolano degli embeddings (punto (d)) $W$ e addestrarle a fare in modo che questi pesi siano minori per figure simili e maggiori per quelle dissimili.\n",
    "    - In questo modo la trasformazione $W$ ci consente di mappare i dati in uno spazio latente produce una suddivisione in classi simili degli oggetti (figura (e)).\n",
    "\n",
    "<img src=\"imgs/metriclearning.PNG\" alt=\"\" width=500>\n",
    "\n",
    "### Funzioni di Loss\n",
    "- Abbiamo visto che tramite degli embeddings possiamo mappare i dati in uno spazio latente\n",
    "    - in particolare calcolando una matrice di trasformazioni $W$ che ci consente di calcolare la distanza di Mahalanobis. \n",
    "    - Questa matrice è calcolata mediante **delle funzioni di loss** che forzino il modello ad avvicinare gli embedding di dati simili e allontanare quelli dissimili.\n",
    "- Esistono diverse funzioni di loss\n",
    "    - **Contrastive Loss**. Data una coppia di dati $(x_1, x_2)$ con rispettive etichette $y_1$ e $y_2$ che potrebbero essere ugali o meno (cioè non sappiamo se $y_1 = y_2$), la loss è definita come\n",
    "    $$\n",
    "        L = \\mathbb{I}_{y_1=y_2} \\vert \\vert W_{\\theta} x_1 - W_{\\theta}x_2 \\vert \\vert_2^2 + \\mathbb{I}_{y_1 \\neq y_2} \\max\\left(0, \\alpha - \\vert \\vert W_{\\theta} x_1 - W_{\\theta}x_2 \\vert \\vert_2^2 \\right)\n",
    "    $$\n",
    "    dove $\\mathbb{I}_A$ è la funzione indicatriceche vale 1 se la proposizione A è vera, 0 altrimenti. \n",
    "    - Pertanto la loss è composta da due termini\n",
    "        - il primo termine è la distanza euclidea fra i due embedding, moltiplicata per un fattore $\\mathbb{I}_{y_1=y_2} \\neq 0$ solo se i due dati sono simili.\n",
    "        - il secondo termine è il massimo fra 0 e la distanza euclidea fra i due embedding se i due dati sono dissimili.\n",
    "    - Intuitivamente poiché voglio minimizzare la loss $L$ \n",
    "        - se $L = $ distanza tra due embeddings simili, minimizzandola otterrò una distanza minore fra i due embedding\n",
    "        - se $L = $ distanza tra due embeddings dissimili, minimizzandola otterrò una distanza maggiore fra i due embedding. Questo perché tanto più vicini sono i due elementi nello spazio originale, quanto più i loro embeddings saranno distanti.\n",
    "        Infatti la distanza euclidea al quadrato ($\\vert \\vert \\cdot \\vert \\vert_2^2$) sarà molto prossima a 0 e dunque $\\alpha - \\vert \\vert \\cdot \\vert \\vert_2^2$ sarà molto grande, e poiché devo prendere il massimo fra quello e 0 prenderò quello. \n",
    "            - $\\alpha$ è un iperparametro che indica la distanza minima che voglio fra due embedding di elementi dissimili. Tipicamente uguale a 1 o 0.1.\n",
    "    - **Triplete Loss**. Funzione molto più efficiente, di contrastive loss (in essa infatti si confrontavano sempre due coppie di elementi, dunque si andava a visualizzare la distanza relativa fra i due elementi). In questo caso, infatti, piuttosto che considerare coppie di elementi, si agisce su delle triplette $x_a, x_p, x_n$ con $y_a = y_p \\neq y_n$ (cioè i primi due sono simili mentre primo e terzo no).\n",
    "        - Si cerca di minimizzare una loss fatta cosi\n",
    "        $$\n",
    "        L = \\text{max}\\left( 0, \\vert \\vert W_\\theta x_a - W_\\theta x_p \\vert \\vert_2^2 - \\vert \\vert W_\\theta x_a - W_\\theta x_n \\vert \\vert_2^2 + \\alpha \\right)\n",
    "        $$\n",
    "        - Due elementi sono considerati dissimili se la loro loss è maggiore di $\\alpha$.\n",
    "\n",
    "## Information Retrieval con Metric Learning\n",
    "- L'obbiettivo dell'information retrieval è quello di trovare i documenti più rilevanti per una determinata query. \n",
    "- Formalmente data una query $S \\in \\mathbb{S}$ tra un insieme di documenti $\\mathbb{T} = \\left\\{ t_1, t_2, \\dots, t_n \\right\\}$\n",
    "    - È necessario trovare una funzione **di ranking** $f(s,t)$ che restituisca uno score di similarità tra una query e un documento.\n",
    "    - Tipicamente si ha $f(s,t) = g(\\psi(s), \\phi(t), \\eta(s,t))$ dove\n",
    "        - $\\psi, \\phi$ sono funzioni che estraggono le feature rispettivamente di query e documenti\n",
    "        - $\\eta$ è una funzione di interazione che modella le relazioni fra query e documento\n",
    "    - Normalmente $\\psi, \\phi, \\eta$ sono transformers (dunque definiti manualmente) e $g$ è un modello di machine learning.\n",
    "\n",
    "- Occorre capire queste funzioni $\\psi, \\phi, \\eta$ come interagiscono con $g$. \n",
    "- Esistono due modi principali\n",
    "    - **Representation-Focused** (in figura (a)). In questo caso ignoro la funzione di interazione $\\eta$ e modello $g$, ad esempio come la distanza euclidea fra $\\psi(s)$ e $\\phi(t)$. Se ad esempio $\\psi(s)$ fosse una trasformazione di un testo e $\\phi(t)$ la trasformazione di un'immagine, allora $g$ potrebbe essere una metrica di score per valutare la similarità fra testo della query e immagine.\n",
    "        - *Estremamente efficiente*. Calcolare distanze euclidee, anche su spazi latenti di centinaia di migliaia di oggetti, costa molto poco.\n",
    "        - *Meno accurate di altri modelli*. Non si riesce a modellare la relazione fra query e documento dal momento che non si considera $\\eta$.\n",
    "\n",
    "    - **Interaction-Focused** (in figura (b)). In essa non si definiscono esplicitamente $\\phi$ e $\\psi$ bensì si modella solo l'interazione.\n",
    "        - Passo il testo $s$ l'immagine $t$ ad una rete neurale che a partire da questi tira fuori uno score.\n",
    "    \n",
    "<img src=\"imgs/interazione.PNG\" alt=\"\" width=400>\n",
    "\n",
    "## Cross-Modal Retrieval\n",
    "- Consiste nell'utlizzare una query di un certo tipo di dato per recuperare documenti di altre modalità \n",
    "    - es. text-to-image, image-to-text, text-to-audio, ecc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpuNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached faiss-gpu-1.7.1.post2.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.post1.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.0.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.5.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.post2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [7 lines of output]\n",
      "      running egg_info\n",
      "      creating C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-pip-egg-info-608wz4lm\\faiss_cpu.egg-info\n",
      "      writing C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-pip-egg-info-608wz4lm\\faiss_cpu.egg-info\\PKG-INFO\n",
      "      writing dependency_links to C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-pip-egg-info-608wz4lm\\faiss_cpu.egg-info\\dependency_links.txt\n",
      "      writing top-level names to C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-pip-egg-info-608wz4lm\\faiss_cpu.egg-info\\top_level.txt\n",
      "      writing manifest file 'C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-pip-egg-info-608wz4lm\\faiss_cpu.egg-info\\SOURCES.txt'\n",
      "      error: package directory 'C:\\Users\\pnmat\\AppData\\Local\\Temp\\pip-install-x1cj4w0h\\faiss-gpu_5cccf06983ac44288f47ea47813b8b45\\faiss\\faiss\\python' does not exist\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch==2.0.1->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytorch-metric-learning in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-metric-learning) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-metric-learning) (1.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-metric-learning) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-metric-learning) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->pytorch-metric-learning) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->pytorch-metric-learning) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->pytorch-metric-learning) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: umap-learn in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (1.11.1)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (0.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from numba>=0.49->umap-learn) (0.40.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pynndescent>=0.5->umap-learn) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\pnmat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-gpu\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install pytorch-metric-learning\n",
    "%pip install umap-learn\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with BERT and Transformers\n",
    "- Si usa la libreria **huggingface transformers** \n",
    "    - Ha internamente implementati un sacco di modelli di cui possiamo scaricare anche i pesi. \n",
    "    - Sono modelli addestrati su dataset enormi e che hanno raggiunto performance molto alte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scarichiamo i pesi e definiamo i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pnmat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using a model of type big_bird to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BertModel: ['cls.predictions.bias', 'bert.pooler.bias', 'bert.pooler.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BigBirdTokenizer\n",
    "\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights) # serve per tokenizzare le frasi \n",
    "bert_model = BertModel.from_pretrained(pretrained_weights)\n",
    "\n",
    "bigbird_pretrained_weights = 'google/bigbird-roberta-base'\n",
    "bigbird_tokenizer = BigBirdTokenizer.from_pretrained(bigbird_pretrained_weights)\n",
    "bigbird_model = BertModel.from_pretrained(bigbird_pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota che big-bird, essendo più ottimizzato, può arrivare a sequenze fino a 4096 parole, mentre bert solo 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is trained for sequences up to 512\n",
      "BigBird is trained for sequences up to 4096\n"
     ]
    }
   ],
   "source": [
    "print(f'BERT is trained for sequences up to {bert_model.config.max_position_embeddings}')\n",
    "print(f'BigBird is trained for sequences up to {bigbird_model.config.max_position_embeddings}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito è mostrato come funziona il tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is tokenized into: ['a', 'sample', 'input', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"A sample input text.\"\n",
    "print(f\"The text is tokenized into: {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso lo usiamo per passare dei token ai modelli.\n",
    "Si usano dei token speciali CLS e SEP per indicare l'inizio e la fine della frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CLS and SEP tokens are added at the start and end of the sequence respectively\n",
      "Each token is assigned an input ID: tensor([[ 101, 1037, 7099, 7953, 3793, 1012,  102]])\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(\"the CLS and SEP tokens are added at the start and end of the sequence respectively\")\n",
    "print(f\"Each token is assigned an input ID: {encoded_input['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When no heads are used, the output is split in two parts: \n",
      "- The ouput relative the  special CLS token (which is usually used for calssification)\n",
      "- One embbedding for each token in the sentence in input which represents a latent contextualized representation\n",
      "\n",
      "Pooled CLS token output: tensor([[-8.8370e-01, -3.8061e-01, -8.9286e-01,  7.7327e-01,  7.2765e-01,\n",
      "         -2.9319e-01,  8.9472e-01,  2.5960e-01, -8.2660e-01, -9.9997e-01,\n",
      "         -6.3866e-01,  9.1840e-01,  9.6854e-01,  4.1925e-01,  9.1151e-01,\n",
      "         -7.1414e-01, -5.3785e-01, -6.8792e-01,  3.4804e-01, -4.8597e-01,\n",
      "          5.8008e-01,  9.9998e-01, -2.2163e-01,  3.7827e-01,  4.3789e-01,\n",
      "          9.8171e-01, -7.5443e-01,  9.0951e-01,  9.3570e-01,  7.1031e-01,\n",
      "         -6.1796e-01,  2.9801e-01, -9.8548e-01, -2.4025e-01, -9.1048e-01,\n",
      "         -9.8896e-01,  4.3167e-01, -6.6727e-01, -9.1795e-02,  1.9245e-02,\n",
      "         -9.0559e-01,  2.9392e-01,  9.9995e-01, -3.4755e-01,  5.1603e-01,\n",
      "         -4.2334e-01, -1.0000e+00,  3.1472e-01, -8.7659e-01,  9.4562e-01,\n",
      "          9.0653e-01,  8.5650e-01,  1.8250e-01,  5.9669e-01,  5.0973e-01,\n",
      "         -1.4548e-01, -1.2181e-01,  2.6751e-01, -2.3491e-01, -6.3227e-01,\n",
      "         -6.7548e-01,  3.2005e-01, -8.5199e-01, -8.8611e-01,  9.3509e-01,\n",
      "          7.0312e-01, -3.2768e-01, -4.5171e-01, -1.3120e-01, -1.7422e-03,\n",
      "          8.5149e-01,  1.8135e-01, -2.4548e-01, -7.3278e-01,  7.2446e-01,\n",
      "          3.9460e-01, -6.8279e-01,  1.0000e+00, -6.4104e-01, -9.6932e-01,\n",
      "          8.6139e-01,  8.0032e-01,  6.5374e-01, -4.9924e-01,  3.9520e-01,\n",
      "         -1.0000e+00,  7.3656e-01, -3.1876e-01, -9.8116e-01,  1.9545e-01,\n",
      "          6.4584e-01, -3.0685e-01,  4.9938e-01,  6.5201e-01, -6.6020e-01,\n",
      "         -4.9896e-01, -4.2957e-01, -8.2737e-01, -2.9168e-01, -4.8069e-01,\n",
      "          7.2640e-02, -3.3539e-01, -4.2406e-01, -4.3229e-01,  3.1679e-01,\n",
      "         -5.0026e-01, -4.6064e-01,  3.5291e-01,  3.9402e-01,  6.6578e-01,\n",
      "          4.9091e-01, -3.7230e-01,  5.3100e-01, -9.4234e-01,  6.7053e-01,\n",
      "         -3.6116e-01, -9.8312e-01, -6.3068e-01, -9.8319e-01,  6.7226e-01,\n",
      "         -3.8604e-01, -3.3318e-01,  9.4089e-01, -8.9423e-02,  3.1855e-01,\n",
      "         -6.1642e-02, -9.3321e-01, -1.0000e+00, -7.4695e-01, -6.8748e-01,\n",
      "         -2.2238e-01, -3.9953e-01, -9.5592e-01, -9.3952e-01,  6.5166e-01,\n",
      "          9.3759e-01,  2.1794e-01,  9.9976e-01, -3.9873e-01,  9.2687e-01,\n",
      "         -4.7667e-01, -7.5085e-01,  6.8699e-01, -4.8463e-01,  8.0482e-01,\n",
      "          3.4703e-01, -4.9748e-01,  3.5809e-01, -3.5484e-01,  5.6599e-01,\n",
      "         -7.3772e-01, -3.4883e-01, -8.1030e-01, -9.0407e-01, -2.7951e-01,\n",
      "          9.3394e-01, -6.9317e-01, -9.4493e-01, -2.3208e-01, -3.1166e-01,\n",
      "         -5.6645e-01,  8.4064e-01,  7.9172e-01,  4.4596e-01, -4.0581e-01,\n",
      "          5.4734e-01,  4.3717e-01,  5.9276e-01, -8.4247e-01, -2.0351e-01,\n",
      "          4.1043e-01, -4.4409e-01, -8.2580e-01, -9.7736e-01, -4.9818e-01,\n",
      "          4.8746e-01,  9.8604e-01,  6.3384e-01,  3.7714e-01,  8.3858e-01,\n",
      "         -3.7154e-01,  8.2758e-01, -9.3240e-01,  9.7136e-01, -3.1363e-01,\n",
      "          3.5177e-01, -2.0014e-01,  2.6668e-01, -8.4929e-01,  3.7200e-02,\n",
      "          8.4420e-01, -6.8843e-01, -7.4577e-01, -1.5799e-01, -5.0373e-01,\n",
      "         -5.2067e-01, -7.6435e-01,  4.7821e-01, -4.4787e-01, -3.7316e-01,\n",
      "         -2.0104e-01,  8.7391e-01,  9.7475e-01,  8.0882e-01,  4.1720e-01,\n",
      "          6.2554e-01, -9.0032e-01, -5.1260e-01,  2.8936e-01,  2.3333e-01,\n",
      "          3.1296e-01,  9.9233e-01, -6.4737e-01, -3.0392e-01, -8.9213e-01,\n",
      "         -9.7548e-01,  1.1797e-01, -8.8467e-01, -2.9646e-01, -6.6937e-01,\n",
      "          6.7187e-01, -3.0623e-01,  5.7953e-01,  4.0282e-01, -9.7510e-01,\n",
      "         -7.0224e-01,  4.4157e-01, -4.3638e-01,  4.5280e-01, -3.2682e-01,\n",
      "          5.8758e-01,  9.0741e-01, -5.1794e-01,  6.9017e-01,  9.0880e-01,\n",
      "         -8.7984e-01, -8.0904e-01,  8.1824e-01, -3.4145e-01,  7.7906e-01,\n",
      "         -7.0466e-01,  9.8574e-01,  9.0160e-01,  7.3528e-01, -8.7233e-01,\n",
      "         -7.1211e-01, -8.3247e-01, -7.0445e-01, -2.8621e-01, -1.0079e-01,\n",
      "          9.1676e-01,  6.6201e-01,  4.6435e-01,  2.5234e-01, -7.1590e-01,\n",
      "          9.9491e-01, -4.1159e-01, -9.4827e-01, -5.3617e-01, -3.0136e-01,\n",
      "         -9.8432e-01,  8.9830e-01,  3.5678e-01,  5.7166e-01, -5.6225e-01,\n",
      "         -7.1427e-01, -9.5145e-01,  8.0487e-01,  2.0504e-01,  9.7509e-01,\n",
      "         -2.0048e-01, -9.1477e-01, -7.4435e-01, -9.1472e-01,  2.4255e-02,\n",
      "         -2.9796e-01, -5.9124e-01, -4.5492e-02, -9.4852e-01,  6.1515e-01,\n",
      "          6.2729e-01,  4.6908e-01, -8.1054e-01,  9.9732e-01,  1.0000e+00,\n",
      "          9.5328e-01,  8.3102e-01,  7.8750e-01, -9.9977e-01, -4.4779e-01,\n",
      "          9.9999e-01, -9.8637e-01, -1.0000e+00, -9.3697e-01, -7.6730e-01,\n",
      "          2.8056e-01, -1.0000e+00, -2.9970e-01, -9.9931e-02, -8.7252e-01,\n",
      "          7.8092e-01,  9.7251e-01,  9.8496e-01, -1.0000e+00,  8.1567e-01,\n",
      "          9.1940e-01, -6.8867e-01,  9.3568e-01, -5.6449e-01,  9.6999e-01,\n",
      "          6.3661e-01,  6.0999e-01, -2.3656e-01,  3.5034e-01, -9.5415e-01,\n",
      "         -8.2072e-01, -6.9429e-01, -7.3214e-01,  9.9680e-01,  1.9742e-01,\n",
      "         -7.7353e-01, -8.1587e-01,  4.7243e-01, -2.3876e-01,  5.5214e-02,\n",
      "         -9.5076e-01, -2.9467e-01,  6.2187e-01,  7.8331e-01,  2.8787e-01,\n",
      "          3.1253e-01, -6.6147e-01,  4.3855e-01,  1.7385e-01,  2.5300e-01,\n",
      "          7.1817e-01, -9.2386e-01, -4.0275e-01, -6.9085e-01,  1.0819e-01,\n",
      "         -6.4881e-01, -9.6012e-01,  9.4097e-01, -4.5672e-01,  9.2174e-01,\n",
      "          1.0000e+00,  4.7902e-01, -8.2923e-01,  6.5236e-01,  3.7969e-01,\n",
      "         -5.9929e-01,  1.0000e+00,  8.0115e-01, -9.7420e-01, -6.0039e-01,\n",
      "          6.7926e-01, -6.3797e-01, -6.5674e-01,  9.9914e-01, -3.3530e-01,\n",
      "         -7.3675e-01, -5.8371e-01,  9.7415e-01, -9.8839e-01,  9.9519e-01,\n",
      "         -8.2740e-01, -9.4540e-01,  9.4229e-01,  9.1807e-01, -7.3868e-01,\n",
      "         -6.9629e-01,  2.4448e-01, -7.1999e-01,  4.8007e-01, -9.3261e-01,\n",
      "          7.5434e-01,  4.8616e-01, -1.5139e-01,  8.4074e-01, -8.0474e-01,\n",
      "         -6.4904e-01,  4.1957e-01, -7.6286e-01, -2.3846e-01,  9.1406e-01,\n",
      "          5.9201e-01, -4.0322e-01,  4.0991e-02, -4.1065e-01, -3.1240e-01,\n",
      "         -9.5288e-01,  6.2477e-01,  1.0000e+00, -3.8123e-01,  7.8668e-01,\n",
      "         -5.0352e-01,  2.0576e-02,  1.3897e-01,  6.0639e-01,  5.8299e-01,\n",
      "         -3.7621e-01, -7.6412e-01,  8.3935e-01, -9.4418e-01, -9.8436e-01,\n",
      "          6.8795e-01,  2.8006e-01, -3.3482e-01,  9.9999e-01,  5.4561e-01,\n",
      "          2.4292e-01,  3.9157e-01,  9.7659e-01, -3.0590e-03,  3.3937e-01,\n",
      "          8.7849e-01,  9.7171e-01, -3.2731e-01,  6.9023e-01,  8.2546e-01,\n",
      "         -9.0800e-01, -1.6983e-01, -6.6457e-01, -6.3751e-04, -9.1337e-01,\n",
      "          4.9377e-02, -9.4252e-01,  9.6764e-01,  9.3185e-01,  4.6952e-01,\n",
      "          2.5243e-01,  8.0085e-01,  1.0000e+00, -6.1882e-01,  6.2291e-01,\n",
      "         -3.7070e-01,  8.1545e-01, -9.9974e-01, -7.7407e-01, -4.4217e-01,\n",
      "         -2.2302e-01, -8.2363e-01, -4.5109e-01,  4.1270e-01, -9.5233e-01,\n",
      "          8.6459e-01,  6.2128e-01, -9.8022e-01, -9.8730e-01, -3.1514e-01,\n",
      "          8.5584e-01,  5.2790e-02, -9.8319e-01, -7.5785e-01, -6.1981e-01,\n",
      "          7.9924e-01, -4.3284e-01, -9.2673e-01, -4.2459e-01, -4.6569e-01,\n",
      "          4.6919e-01, -2.4696e-01,  6.9028e-01,  8.5265e-01,  6.9670e-01,\n",
      "         -8.3377e-01, -4.2417e-01, -2.1231e-01, -8.3741e-01,  8.2628e-01,\n",
      "         -7.4133e-01, -9.2521e-01, -2.6913e-01,  1.0000e+00, -4.2754e-01,\n",
      "          8.9985e-01,  6.4656e-01,  6.7685e-01, -2.6095e-01,  3.5327e-01,\n",
      "          9.2262e-01,  4.5938e-01, -6.8841e-01, -8.9788e-01, -5.7103e-01,\n",
      "         -3.9889e-01,  6.7246e-01,  6.3114e-01,  6.2801e-01,  7.6479e-01,\n",
      "          7.6865e-01,  1.4982e-01, -8.6210e-02,  1.5045e-01,  9.9922e-01,\n",
      "         -2.5767e-01, -6.8403e-02, -3.8420e-01, -2.5723e-01, -3.8620e-01,\n",
      "         -3.4021e-01,  1.0000e+00,  4.3189e-01,  6.0197e-01, -9.8626e-01,\n",
      "         -8.7686e-01, -8.8503e-01,  1.0000e+00,  7.5653e-01, -6.7010e-01,\n",
      "          7.1620e-01,  7.0490e-01, -2.5207e-01,  7.1660e-01, -2.2310e-01,\n",
      "         -3.0084e-01,  2.8953e-01,  2.6428e-01,  9.3371e-01, -6.5666e-01,\n",
      "         -9.6106e-01, -6.3152e-01,  5.0012e-01, -9.5388e-01,  9.9989e-01,\n",
      "         -7.0897e-01, -3.1732e-01, -4.6418e-01, -2.9461e-01,  4.0708e-01,\n",
      "          5.3844e-02, -9.6833e-01, -2.2881e-01,  3.4698e-01,  9.4010e-01,\n",
      "          2.8552e-01, -6.4721e-01, -9.1032e-01,  8.7205e-01,  8.5971e-01,\n",
      "         -9.0536e-01, -9.1821e-01,  9.4639e-01, -9.6136e-01,  6.1942e-01,\n",
      "          1.0000e+00,  5.4457e-01,  3.9146e-01,  3.2605e-01, -2.8321e-01,\n",
      "          4.7359e-01, -5.7857e-01,  7.2938e-01, -9.3148e-01, -3.4854e-01,\n",
      "         -2.6667e-01,  3.4048e-01, -1.8167e-01, -3.4264e-01,  6.7242e-01,\n",
      "          2.4982e-01, -6.6243e-01, -6.1242e-01, -2.4237e-01,  5.0217e-01,\n",
      "          8.2890e-01, -3.3110e-01, -2.7669e-01,  2.8424e-01, -3.1066e-01,\n",
      "         -8.5977e-01, -4.5935e-01, -5.3617e-01, -9.9999e-01,  6.5893e-01,\n",
      "         -1.0000e+00,  5.4230e-01,  1.9512e-01, -2.3983e-01,  8.7447e-01,\n",
      "          3.9629e-01,  7.0015e-01, -7.0621e-01, -8.9954e-01,  2.6358e-01,\n",
      "          7.3983e-01, -3.6903e-01, -6.5875e-01, -5.7759e-01,  4.2993e-01,\n",
      "         -1.0694e-01,  1.1103e-01, -5.9973e-01,  7.4996e-01, -3.3536e-01,\n",
      "          1.0000e+00,  2.5835e-01, -6.6824e-01, -9.5010e-01,  2.7954e-01,\n",
      "         -2.6266e-01,  1.0000e+00, -8.8537e-01, -9.5190e-01,  3.4469e-01,\n",
      "         -6.8549e-01, -7.9142e-01,  4.9352e-01,  2.3894e-01, -7.8031e-01,\n",
      "         -9.5093e-01,  9.3182e-01,  9.3210e-01, -6.5407e-01,  6.0882e-01,\n",
      "         -4.0883e-01, -5.2663e-01,  2.1178e-01,  8.8335e-01,  9.8162e-01,\n",
      "          6.4818e-01,  8.4829e-01,  3.2112e-01, -9.6449e-02,  9.6544e-01,\n",
      "          4.1434e-01,  3.4021e-01,  3.0092e-01,  1.0000e+00,  5.3604e-01,\n",
      "         -9.2510e-01, -1.6315e-01, -9.7419e-01, -3.2705e-01, -9.3427e-01,\n",
      "          4.2131e-01,  3.9715e-01,  8.5709e-01, -4.0222e-01,  9.4626e-01,\n",
      "         -8.5107e-01,  7.0114e-02, -7.3674e-01, -6.7789e-01,  5.6693e-01,\n",
      "         -9.2053e-01, -9.7944e-01, -9.8543e-01,  6.9259e-01, -4.7447e-01,\n",
      "         -1.0573e-01,  3.4072e-01,  8.4854e-02,  5.2259e-01,  5.1511e-01,\n",
      "         -1.0000e+00,  9.1371e-01,  4.6629e-01,  8.9708e-01,  9.3870e-01,\n",
      "          6.5669e-01,  6.1994e-01,  3.2946e-01, -9.8068e-01, -9.5911e-01,\n",
      "         -4.8747e-01, -4.3930e-01,  6.9768e-01,  6.6258e-01,  8.2177e-01,\n",
      "          5.8009e-01, -5.9698e-01, -2.2924e-01, -7.0403e-01, -4.7595e-01,\n",
      "         -9.8934e-01,  5.7149e-01, -6.4965e-01, -9.1961e-01,  9.4804e-01,\n",
      "          3.2094e-03, -1.2968e-01, -9.6659e-02, -8.6609e-01,  8.9247e-01,\n",
      "          6.8413e-01,  6.3050e-02,  2.0626e-01,  4.1792e-01,  8.4717e-01,\n",
      "          9.4326e-01,  9.8019e-01, -8.2110e-01,  7.4621e-01, -7.2232e-01,\n",
      "          4.3626e-01,  6.7295e-01, -9.1131e-01,  3.7415e-01,  4.5417e-01,\n",
      "         -3.6066e-01,  3.3205e-01, -2.8846e-01, -9.5167e-01,  8.0835e-01,\n",
      "         -3.6484e-01,  5.2491e-01, -4.4749e-01,  3.8096e-02, -4.8692e-01,\n",
      "         -4.2585e-01, -8.3899e-01, -6.8128e-01,  6.3742e-01,  4.8183e-01,\n",
      "          8.6138e-01,  8.0894e-01, -1.5836e-01, -8.0638e-01, -2.7609e-01,\n",
      "         -7.7445e-01, -9.1757e-01,  9.1725e-01, -1.5810e-01, -1.9575e-01,\n",
      "          6.8970e-01,  7.5529e-02,  8.8617e-01,  2.2492e-01, -4.5472e-01,\n",
      "         -3.7229e-01, -7.1644e-01,  7.3508e-01, -5.9415e-01, -7.1738e-01,\n",
      "         -6.0371e-01,  7.2052e-01,  4.4790e-01,  9.9998e-01, -8.3096e-01,\n",
      "         -9.2297e-01, -4.6277e-01, -4.6075e-01,  4.5943e-01, -6.5074e-01,\n",
      "         -1.0000e+00,  4.4400e-01, -6.9025e-01,  6.9068e-01, -8.1211e-01,\n",
      "          8.4943e-01, -8.2986e-01, -9.7565e-01, -3.4039e-01,  4.9806e-01,\n",
      "          8.2149e-01, -5.4842e-01, -7.7687e-01,  5.6390e-01, -2.6386e-01,\n",
      "          9.7887e-01,  7.5353e-01, -6.5291e-01, -5.6342e-02,  6.9483e-01,\n",
      "         -8.4682e-01, -7.0104e-01,  8.8102e-01]], grad_fn=<TanhBackward0>), shape: torch.Size([1, 768])\n",
      "Contextualized embeddings: tensor([[[-0.7700, -0.1339, -0.6115,  ..., -0.3188,  0.1008,  0.6080],\n",
      "         [-0.3071, -0.4836, -0.3850,  ..., -0.3985,  0.5804,  0.5132],\n",
      "         [-0.0351, -0.4324,  0.0530,  ..., -0.5289, -0.0845, -0.2632],\n",
      "         ...,\n",
      "         [-0.1285, -0.0382,  0.3915,  ..., -0.4591, -0.0536,  0.2246],\n",
      "         [-1.1617, -1.0774, -0.2419,  ...,  0.6520,  0.4441, -0.3359],\n",
      "         [ 0.8983,  0.0095, -0.4727,  ...,  0.4925, -0.5942, -0.1789]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), shape: torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "output = bert_model(**encoded_input)\n",
    "\n",
    "print(\"\"\"\n",
    "When no heads are used, the output is split in two parts: \n",
    "- The ouput relative the  special CLS token (which is usually used for calssification)\n",
    "- One embbedding for each token in the sentence in input which represents a latent contextualized representation\n",
    "\"\"\")\n",
    "\n",
    "cls = output['pooler_output']\n",
    "hidden_state = output['last_hidden_state']\n",
    "print(f\"Pooled CLS token output: {cls}, shape: {cls.shape}\")\n",
    "print(f\"Contextualized embeddings: {hidden_state}, shape: {hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BERT nlp power\n",
    "Possiamo usare una `pipeline` per adattare BERT ai task più disparati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso vogliamo che BERT faccia **Masked Language Modelling**, ossia predire che parole vanno inserite al posto di token speciali come `<mask>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098354</td>\n",
       "      <td>928</td>\n",
       "      <td>London</td>\n",
       "      <td>The capital of UK is London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059162</td>\n",
       "      <td>9652</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>The capital of UK is Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028790</td>\n",
       "      <td>8353</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>The capital of UK is Birmingham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token    token_str                         sequence\n",
       "0  0.098354    928       London      The capital of UK is London\n",
       "1  0.059162   9652    Edinburgh   The capital of UK is Edinburgh\n",
       "2  0.028790   8353   Birmingham  The capital of UK is Birmingham"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "masked_language_modeling = pipeline(\"fill-mask\", top_k=3)\n",
    "masked_phrase = f\"The capital of UK is <mask>\"\n",
    "\n",
    "pd.DataFrame(masked_language_modeling(masked_phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito, invece, chiediamo a BERT di fare **Sentiment Analysis**. Gli passiamo una frase e lui ci dice se è positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997871518135071}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"The film was horrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Answering** con BERT. Gli passiamo un contesto e una domanda e lui ci restituisce la risposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Thursday\n",
      "Score: 0.8549147844314575\n"
     ]
    }
   ],
   "source": [
    "question_answering = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "The US has passed the peak on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month. The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world. At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors. \n",
    "\"\"\"\n",
    "\n",
    "question = \"When Trump will announce the new guidelines?\"\n",
    "\n",
    "result = question_answering(question=question, context=context)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning\n",
    "- Per fare metric learning si usa ``pytorch-metric-learning`` che consente di usare un sacco di LOSS già implementate.\n",
    "    - Una loss function è una funzione che consente di forzare la distanza euclidea fra le trasformazioni dei dati (embeddings) in input ad essere tanto più grande quanto i dati in input siano dissimili.\n",
    "- Vogliamo fornire al modello delle immagini di numeri scritti a mano\n",
    "    - l'obbiettivo è quello di porre vicini numeri simili e lontani quelli dissimili come mostrato in figura\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*E6UUEmxKp5ZTRgCRNbIP-g.png\" alt=\"metric learning mnist\" width=\"300\"/>\n",
    "\n",
    "- CNN è una Convolutional Neural Netowrk.  \n",
    "    - Sono tipicamente usata nell'elaborazione d'immagini.\n",
    "    - Usano un layer chiamato **convolutional layer** che è un layer che prende in input un'immagine e restituisce un'immagine trasformata tramite funzione convoluzionale.\n",
    "- Addestreremo il modello sul dataset MNIST che contiene immagini di numeri scritti a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim = 128):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(21632, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}:  Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "\n",
    "def visualize_embeddings(embeddings_reduced, labels):\n",
    "  label_set = np.unique(labels)\n",
    "  num_classes = len(label_set)\n",
    "  fig = plt.figure(figsize=(20, 15))\n",
    "  plt.gca().set_prop_cycle(\n",
    "      cycler(\n",
    "          \"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]\n",
    "      )\n",
    "  )\n",
    "  for i in range(num_classes):\n",
    "      idx = (labels == label_set[i])\n",
    "      plt.plot(embeddings_reduced[idx, 0], embeddings_reduced[idx, 1], \".\", markersize=1,label=i)\n",
    "  plt.legend(loc='best',markerscale=10)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "#indico al modello di usare la GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# che trasformazione applicare alle immagini, in particolare si trasformanno in tensori (cioè un array di numeri) e si applica una random erasing (cancello una piccola parte random dell'immagine)\n",
    "mnist_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.RandomErasing(p=0.35)]\n",
    ")\n",
    "\n",
    "# quanti elementi in contemporanea passare alla rete\n",
    "batch_size = 256\n",
    "\n",
    "# load mnist train and test set\n",
    "dataset1 = datasets.MNIST(\".\", train=True, download=True, transform=mnist_transform)\n",
    "dataset2 = datasets.MNIST(\".\", train=False, transform=mnist_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256)\n",
    "\n",
    "\n",
    "# in questo modo dico al modello quanto dev'essere grande lo spazio latente (32) e creo una CNN a cui passo lo spazio latente creato\n",
    "embedding_dim = 32\n",
    "model = CNN(embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proviamo ora a visualizzare lo spazio latente prima dell'addestramento. \n",
    "    - Poiché `embedding_dim=32`, avremo uno spazio in $\\mathbb{R}^{32}$ che non può essere visualizzato in due dimensioni. \n",
    "    - Esistono però degli algoritmi di **dimensionality reduction** che mappano lo spazio latente in $\\mathbb{R}^{2}$ in modo da poterlo visualizzare.\n",
    "        - Useremo **UMAP** ma anche **t-SNE** è un buon algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo tutti gli embeddings \n",
    "embeddings, labels = get_all_embeddings(dataset2, model) # get MNIST test embeddings\n",
    "umap_visualizer = umap.UMAP()\n",
    "\n",
    "# riduco la loro dimensionalità a 2-D per poterli visualizzare\n",
    "embeddings_reduced = umap_visualizer.fit_transform(embeddings.cpu().numpy())\n",
    "labels = labels.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Come mostra l'immagine i numeri sono disposti casualmente nello spazio.\n",
    "- Adesso proviamo ad addestrare il modello\n",
    "    - Lo facciamo per una sola epoca (iterazione), ma potremmo potenzialmente farlo all'infinito se avessimo tempo\n",
    "    - Si esegue idealmente finché non diminuisce la LOSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "margin = 0.1\n",
    "loss_func = losses.TripletMarginLoss(margin=margin)\n",
    "mining_func = miners.TripletMarginMiner(margin=margin, type_of_triplets=\"semihard\")\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "\n",
    "num_epoch = 1\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    test(dataset1, dataset2, model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- già dopo un'epoca lo spazio è molto più ordinato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient retrieval con FAISS\n",
    "- FAISS è una libreria che consente di fare efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 256) [[0.7159888  0.14255933 0.86490315 ... 0.11997068 0.24203518 0.3275863 ]\n",
      " [0.26962274 0.7558935  0.68770736 ... 0.6785032  0.84126323 0.63282984]\n",
      " [0.24630243 0.32650712 0.9753533  ... 0.13200738 0.5722474  0.9182285 ]\n",
      " ...\n",
      " [0.30953985 0.16022821 0.83510387 ... 0.08353953 0.11749248 0.99064505]\n",
      " [0.41456822 0.30173597 0.92696434 ... 0.60261434 0.7870482  0.25074348]\n",
      " [0.97278035 0.813742   0.7845714  ... 0.28141555 0.28312346 0.94374275]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "d = 256 #dimensione dell'embedding\n",
    "N = 100000 #numero di elementi da inserire nel database\n",
    "\n",
    "dataset_embeddings = np.random.rand(N,d).astype('float32')\n",
    "print(dataset_embeddings.shape, dataset_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adesso generiamo delle query casuali\n",
    "    - esse sono anche embeddings in $\\mathbb{R}^{256}$, cioè vettori nello stesso spazio del dataset\n",
    "    - useremo $Q=500$ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 500\n",
    "query_embeddings = np.random.random((Q, d)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive K-NN Nearest Neighboor\n",
    "- La **Naive K-Nearest Neighboor** è un algoritmo che per ogni query calcola la distanza di Mahalanobis fra la query e tutti i dati del dataset e restituisce i $k$ più vicini.\n",
    "- Occorre scegliere un `IndexFlatL2` che è un indice che consente di calcolare la distanza euclidea fra vettori.\n",
    "    - è l'elemento ancora che non ha bisogno di essere spostato \n",
    "    - su di lui si calcolano le distanze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "l2_index = faiss.IndexFlatL2(d)\n",
    "print(l2_index.is_trained) \n",
    "l2_index.add(dataset_embeddings)\n",
    "print(l2_index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
