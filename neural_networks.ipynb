{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "- I calcolatori riescono a risolvere problemi che non sono facilmente risolvibili con algoritmi tradizionali\n",
    "    - Riordinare alfabeticamente una lista di utenti\n",
    "    - Calcolare la media annuale di incassi per ogni categoria di prodotto\n",
    "    - ...\n",
    "- Tuttavia fanno molta difficoltà con task molto semplici per un umano\n",
    "    - Riconoscere una recensione positiva o negativa\n",
    "    - Interpretare testo e discorso\n",
    "- In generale è veramente difficile per i calcolatori riconoscere pattern di alto livello in dati non strutturati (testo, foto, audio, ecc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitazioni del Machine Learning tradizionale\n",
    "- Alla base di un buon modello di ML vi è sempre un **training set appropriato**.\n",
    "    - Centinaia di migliaia di esempi sono richiesti\n",
    "    - Nel learning supervisionato occorre aggiungere delle label agli esempi manualmente. I dati sono stati prodotti durante gli anni ma prima dell'avvento del machine learning nessuno avrebbe pensato di aggiungere delle label.\n",
    "- I modelli di ML hanno bisogno di una **rappresentazione dei dati** appropriata.\n",
    "    - La maggior parte dei modelli di ML hanno bisogno di una rappresentazione dei dati sottoforma di vettori di numeri reali.\n",
    "    - Le feature (ogni elemento del vettore) devono essere significative per il task che si vuole risolvere, cioè devono essere scelte correttamente.\n",
    "- Un modello addestrato è valido solo per il task per cui è stato addestrato, non vi è modo di fare **generalizzazione** facilmente. \n",
    "    - Un modello addestrato a classificare immagini di gatti non può essere usato per classificare immagini di cani."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "- Le reti neurali sono un tipo di modello di ML che cerca di risolvere i problemi sopra elencati.\n",
    "- Sono ispirate al funzionamento del **cervello umano**.\n",
    "    - Sono composte da **neuroni** artificiali che sono collegati tra loro in grandissime reti.\n",
    "    - Ogni neurone è attivato da quelli a cui è collegato quando la loro attivazione supera una certa soglia misurata in peso e thresholds.\n",
    "- L'idea alla base di una NN è quella di computare una funzione $\\mathbf{y} = f(\\mathbf{x})$ dove $\\mathbf{x}$ è un vettore di input e $\\mathbf{y}$ è un vettore di output.\n",
    "- Può essere usato per risolvere problemi tradizionali come la classificazione e predizione.\n",
    "    - Nel caso della classificazione, su $N$ classi di output, il modello restituisce un vettore di dimensione $N$ con la probabilità che l'istanza appartenga a ciascuna delle $N$ classi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodi\n",
    "- Come anticipato le reti neurali sono composte da **neuroni** o **nodi**\n",
    "- In ogni momento un neurone $n$ emette un valore $y_n = f_n(\\mathbf{x})$ con $\\mathbf{x} = (x_1, \\dots, x_m)$ calcolato sui valori di $x$ forniti in input dai neuroni a cui è collegato.  \n",
    "Quanto detto è riassunto nella seguente figura\n",
    "\n",
    "<img src=\"imgs/neuron.PNG\" alt=\"neurone\" width=400>\n",
    "\n",
    "- Ogni input $x_i$ è ricevuto esattamente da un nodo (l'$i$-esimo) e ogni nodo può spedire il proprio $y_n$ a più neuroni a cui è collegato.\n",
    "- Questa operazione apparentemente semplice di calcolo di $y_n$ consente di comprendere relazioni molto profonde fra gli input.\n",
    "\n",
    "### Layers\n",
    "- I nodi sono tipicamente arrangiati in **layers** (strati) e ogni layer è composto da un certo numero di nodi. L'insieme dei layer costituisce una rete neurale.\n",
    "    - Inizialmente i layer erano tipicamente tre:\n",
    "        - **Input layer**: riceve gli input e li passa al layer successivo\n",
    "        - **Hidden layer**: riceve gli input dal layer precedente e li passa al layer successivo\n",
    "        - **Output layer**: riceve gli input dal layer precedente e restituisce l'output della rete neurale\n",
    "\n",
    "\n",
    "        <img src=\"imgs/layers.PNG\" alt=\"layers\" width=500>\n",
    "    \n",
    "    - Ad oggi si è capito che con delle **reti neurali profonde** cioè composte da decine e decine di layer si ottengono risultati migliori.\n",
    "- Gli input del layer $l$ sono tutti gli output degli $l-1$ layer precedenti.\n",
    "    - Il primo layer è quello che riceve dati grezzi (non necessariamente devono seguire qualche pattern o essere strutturati).\n",
    "    - Poi vengono posti degli hidden layer in mezzo che eseguono delle computazioni su quei dati per trovare degli eventuali pattern.\n",
    "    - Infine è posto un layer di output che restituisce il risultato della computazione. \n",
    "        - Questo può cambiare in base al task che si vuole risolvere. Ad esempio nel caso della classificazione questo layer avrà tanti nodi quante sono le classi in cui si vuole classificare l'input e restituirà la probabilità che l'input appartenga a ciascuna delle classi.\n",
    "\n",
    "- L'architettura appena vista è detta **multi-layer perceptor** (MLP) ed è la più semplice architettura di rete neurale.\n",
    "\n",
    "### Funzione dei nodi\n",
    "- Come già detto i nodi eseguono una computazione $f$ sugli input che ricevono. \n",
    "- Questa funzione non è altro che una somma pesata fra il prodotto degli input e dei pesi e un threshold. \n",
    "- In particolare dato un vettore di input $\\mathbf{x} \\in \\mathbb{R}^d$, sia l'output scalare $y_j$ del nodo $j$\n",
    "$$\n",
    "    y_i = \\sigma_j \\left( \\sum_{i=1}^d w_{j,i} \\cdot x_i + b_j \\right)\n",
    "$$\n",
    "\n",
    "- il peso che ha l'arco che collega il nodo $i$ al nodo $j$, chiamato $w_{j,i}$ \n",
    "- lo moltiplichiamo per il valore di input $x_i$ corrispondente \n",
    "- la somma di tutti i prodotti pesati degli input è sommata a un valore $b_j$ chiamato **bias**. Questo bias consente ai neuroni di apprendere la tendenza generale dei dati e di adattarsi a diverse situazioni. Aggiungendo un bias, la rete neurale ha la capacità di imparare relazioni non lineari tra gli input e l'output desiderato.\n",
    "- Infine il risultato della somma è passato ad una funzione $\\sigma_j$ chiamata **funzione di attivazione** che restituisce l'output del neurone $j$.\n",
    "- Questa notazione scalare può essere sintetizzata nella seguente \n",
    "$$\n",
    "    y_i = \\sigma_j \\left( \\mathbf{W} \\cdot \\mathbf{x} + b_j \\right)\n",
    "$$\n",
    "dove $\\mathbf{W}$ è il vettore dei pesi e $\\mathbf{x}$ è il vettore degli input.\n",
    "\n",
    " - Le reti neurali dunque vanno progressivamente a migliorarsi andando a calibrare meglio i pesi della matrice $\\mathbf{W}$ e il bias $b_j$.\n",
    "\n",
    " #### Funzioni di attivazione\n",
    " - Esistono diverse funzioni di attivazione per i neuroni\n",
    "    - tipicamente è più sensato usare funzioni non lineari poiché consentono di apprendere relazioni più complicate fra i dati.\n",
    "- Le funzioni sono\n",
    "    - **Step function** usata nelle prime reti neurali, ma essendo lineare ormai non più usata. Nella sua versione più semplice funziona come segue\n",
    "    $$\n",
    "        \\sigma(x) = \\begin{cases}\n",
    "            1 & \\text{se } x \\geq \\text{soglia} \\\\\n",
    "            0 & \\text{altrimenti}\n",
    "        \\end{cases}\n",
    "    $$\n",
    "    - **Sigmoid function** ad oggi è forse una delle più utilizzate. \n",
    "        - è una funzione non lineare che ha un output compreso fra 0 e 1. \n",
    "        - è una versione continua e più soft della step function\n",
    "        - la sua formulazione è la seguente \n",
    "        \n",
    "    $$\n",
    "        \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "    $$\n",
    "\n",
    "\n",
    "## Come addestrare le reti neurali\n",
    "- Occorre come prima cosa avere dei dati etichettati della forma $(\\mathbf{x}_i, \\mathbf{g}_i)$ dove\n",
    "    - $\\mathbf{x}_i$ è un vettore di input fornito alla rete\n",
    "    - $\\mathbf{g}_i$ è un vettore di output che ci si aspetta venga restituito dalla rete neurale (per guidarla a capire cosa deve fare)\n",
    "- Per ogni input $\\mathbf{x}_i$ la rete neurale restituisce un vettore $\\mathbf{y}_i$ che è la sua predizione.\n",
    "- L'obbiettivo della rete è quello di **minimizzare la differenza* fra $\\mathbf{y}_i$ e $\\mathbf{g}_i$ per ogni $i$.\n",
    "- Per fare ciò si usa una funzione di costo chiamata **loss** $C(\\mathbf{y}_i, \\mathbf{g}_i)$ che misura la differenza fra i due vettori.\n",
    "    - Si calcola su ogni **batch** di training. Un batch è un sottoinsieme dei dati di training che viene usato per addestrare la rete. \n",
    "    - Esistono diverse funzioni di loss che servono a misurare l'accuratezza del modello\n",
    "    - La più semplice è l'**errore quadratico medio** (MSE)\n",
    "    $$\n",
    "        \\text{MSE} = \\frac{1}{2N} \\sum_{i=1}^N \\vert \\vert \\mathbf{y}_i - \\mathbf{g}_i \\vert \\vert^2\n",
    "    $$\n",
    "    - Si può tenere traccia della loss ad ogni iterazione di training e plottarla per vedere se il modello sta migliorando o meno.\n",
    "        <img src=\"imgs/loss.PNG\" alt=\"loss-epoch\" width=300>\n",
    "    - La modalità con cui si minimizza l'errore è chiamata **back-propagation** o discesa del gradiente. È lo stesso approccio usato nel caso della regressione: si calcola una funzione d'errore e si cerca di minimizzarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiti delle reti feed forward\n",
    "Le reti trattate fino ad ora vengono definite **feed forward** poiché l'informazione fluisce in una sola direzione, da sinistra verso destra. Questo tipo di reti hanno dei limiti:\n",
    "- Non consentono di mantere memoria di ciò che è stato visto in precedenza (lo stato).\n",
    "    - Ad esempio se si vuole comprendere il significato di un testo l'ordine delle parole è importante. Con le reti feed-forward questo non si riesce a fare\n",
    "\n",
    "## Reti neurali ricorrenti (RNN)\n",
    "Queste reti introducono il concetto di **ciclo**. Ogni layer non solo può inviare il proprio output ai layer successivi ma anche a se stesso. In questo modo si riesce a mantenere lo stato e a comprendere l'ordine delle cose.\n",
    "- In altre parole si possono avere cicli \n",
    "- I cicli consentono di processare informazioni ottenute in momenti differenti\n",
    "- Le informazioni processate in un ciclo costituiscono uno **stato interno** della rete\n",
    "- L'output di un nodo è funzione sia dll'input che dello stato interno\n",
    "- Tutto questo consente di effettuare **processazioni context-dependent**\n",
    "    - ad esempio predire la prossima parola in un testo richiede la conoscenza di tutte le parole precedenti.\n",
    "\n",
    "In poche parole lo schema è il seguente \n",
    "\n",
    "<img src=\"imgs/rnn.PNG\" alt=\"rnn\" width=180>\n",
    "\n",
    "dove nell'hidden layer è presente un ciclo con un **neurone di contesto** che mantiene lo stato.\n",
    "\n",
    "- Le **context unit** propagano lo stato attraverso un arco di peso $\\mathbf{U}$ all'hidden layer (insieme all'input) e questo viene usato per calcolare l'output del neurone all'iterazione corrente. \n",
    "    - Se definiamo la sequenzialità come la sequenza di $t_0 \\rightarrow t_1 \\rightarrow \\dots \\rightarrow t_n$ allora l'output del neurone $j$ all'iterazione $t$ è dato da\n",
    "    $$\n",
    "    \\mathbf{h}_t = \\sigma_t \\left( \\mathbf{W} \\cdot \\mathbf{x}_t + \\mathbf{U} \\cdot \\mathbf{t}_{t-1} + \\mathbf{b} \\right)\n",
    "    $$\n",
    "    \n",
    "## Attention\n",
    "\n",
    "Ad un certo punto ci si è resi conto che le RNN funzionavano molto bene per task di traduzione automatica del testo basati su un'architettura **encoder-decoder** e reti neurali ricorrenti che funzionava come segue\n",
    "- Una rete chiamata **encoder** prendeva in input una frase in una lingua e la codificava in un vettore di dimensione fissa\n",
    "- Una rete chiamata **decoder** prendeva in input l'output dell'encoder e restituiva una sua rappresentazione in lingua inglese\n",
    "\n",
    "Tuttavia questa architettura faceva difficoltà a tenere in memoria frasi molto lunghe. Questo portava poi il decoder a fare una conversione solo di una parte del testo, sbagliando il risultato finale. Su questo è nato il meccanismo di **attention** che semplicemente consiste nel dare un peso maggiore alle parole più importanti di una frase. In questo modo il decoder può concentrarsi su quelle parole e ignorare le altre.\n",
    "- L'idea pertanto è quella di fornire in input al decoder la somma pesata di tutti gli output dell'encoder ad ogni iterazione.\n",
    "- I pesi assegnati alla sequenza in input vengono imparati durante l'addestramento da una rete neurale chiamata **attention layer** (o **alignment model**).\n",
    "\n",
    "\n",
    "<img src=\"imgs/attention.PNG\" alt=\"rnn\" width=700>\n",
    "\n",
    "In questa immagine vediamo \n",
    "- l'encoder che non è altro che una rete neurale ricorrente. In particolare infatti per ogni input fornito in sequenza temporale (prima $\\mathbf{x}_0$ poi $\\mathbf{x}_1$ e cosi via) i vari neuroni comunicano avanti e indietro (frecce arancioni e viola) per fornire un output.\n",
    "- dall'altro lato il decoder, anch'esso RNN, all'iterazione 3 (vediamo infatti che sta calcolando $\\mathbf{y}'_3$) prende in input tutti gli ouput dell'encoder, ne fa una somma pesata (i cui pesi vengono presi dall'allignment model (attention layer) a destra) e calcola l'output.\n",
    "\n",
    "Supponendo di avere tante iterazioni del decoder quanti sono gli input dell'encoder (cioè la lunghezza dell'input $L$), allora dovremo calcolare circa $L^2$ pesi, poiché per ogni output del decoder dovremo calcolare la somma pesata di tutti gli output dell'encoder.\n",
    "\n",
    "- Come si può vedere l'attention layer è una rete neurale feed forward che prende in input gli output dell'encoder e restituisce i pesi che verranno usati per calcolare la somma pesata. Abbiamo un neurone per ogni output dell'encoder e la modalità di calcolo dei pesi è la seguente (si basa s una funzione di **softmax**) che si calcola cosi\n",
    "$$\n",
    "\\alpha_{(t,i)} = \\frac{exp(e_{(t,i)})}{\\sum_{j} exp(e_{(t,j)})}\n",
    "$$\n",
    "\n",
    "Esistono molti tipi di attention layer, fra cui\n",
    "- **Concatenative Attention** calcolato come segue\n",
    "    $$\n",
    "    e_{(t,i)} = \\mathbf{v}^T \\tanh \\left( \\mathbf{W} \\left[ \\mathbf{h}_{(t)} ; \\mathbf{y}(i) \\right] \\right)\n",
    "    $$\n",
    "    dove $\\mathbf{v}$ è un parametro di scaling e ; è l'operatore di concatenazione.\n",
    "\n",
    "\n",
    "- **Multiplicative Attention** che nasce per accellerare i calcoli della concatenative attention. Non usa più la tangente ed è ottimizzata per le matrici.\n",
    "    $$\n",
    "    e_{(t,i)} = \\mathbf{h}_{(t)}^T \\mathbf{W} \\mathbf{y}(i)\n",
    "    $$\n",
    "\n",
    "- la più semplice è quella **dot-product** che calcola i pesi semplificando il prodotto matriciale per $\\mathbf{W}$ come nel caso di multiplicative attention. Si calcola come segue\n",
    "$$\n",
    "e_{(t,i)} = \\mathbf{h}_{(t)}^T \\mathbf{y}(i)\n",
    "$$\n",
    "\n",
    "\n",
    "## Attention is all you need: Transformer\n",
    "- Nel 2017 ci si è resi conto che le reti neurali ricorrenti non erano necessarie per il meccanismo di Attention. \n",
    "    - I layer ricorrenti generano dei problemi essendo poco ottimizzati per il calcolo parallelo. Infatti per ogni input devo aspettare che tutti gli input precedenti siano stati processati.\n",
    "- Nasce cosi il modello **Transformer** schematizzato nella seguente immagine\n",
    "\n",
    "<img src=\"imgs/transformer.png\" alt=\"\" width=400>\n",
    "\n",
    "- L'idea è quella di togliere le reti neurali ricorrenti ma mantenere l'attention\n",
    "- Questo è ottenuto aggiungendo un layer chiamato **Multi-Head Attention** che non fa altro che calcolare l'attention dot product senza usare reti ricorrenti.\n",
    "- A sinistra abbiamo un encoder e a destra un decoder.\n",
    "- Possiamo avere più blocchi di questo tipo concatenati (da qui la notazione in figura Nx), in modo da avere una rete più profonda.\n",
    "\n",
    "### Input di un transformer \n",
    "- Data una frase di $L$ parole scomposta nei rispettivi $L$ token, allora a ciascun token dovrò associare un **embedding**. Questo embedding è un vettore di dimensione $d$ che rappresenta il token.\n",
    "    - Per ottenere un embedding si usa un layer chiamato **embedding layer** (in figura **input embedding**) che non è altro che un layer prende in input un token e restituisce il suo embedding. \n",
    "    - Questo layer combina tutti gli embedding in un'unica matrice $\\mathbf{Z}$ di dimensione $L \\times d$ dove $L$ è la lunghezza della frase.\n",
    "        - Poiché $L$ è costante, se l'input dovesse essere più corto si aggiungono dei valori di padding\n",
    "    - Il problema a questo punto è che la rete, non essendo più ricorrente, perde il concetto di *sequenzialità* ovvero non si riesce più a capire se un input viene prima o dopo di un'altro.\n",
    "        - Per questa ragione è stato aggiunto un altro layer chiamato **positional embedding** che dice, per ogni token in input, qual'è la sua posizione nella frase.\n",
    "        - Per fare questo il positional embedding calcola una nuova matrice $\\mathbf{P}$ contenente questi positional embedding.\n",
    "        - La matrice $\\mathbf{P}$ può essere dedotta dalla retre stessa o calcolata a priori con la seguente formula: detta $i$ la posizione della parola nella frase e $j$ l'indice del word embedding, allora\n",
    "        $$\n",
    "            p_{i,j} = \\begin{cases}\n",
    "                \\sin \\left( \\frac{1}{10000 \\frac{j}{d}} \\right) \\quad \\text{se } j \\text{ è pari} \\\\\n",
    "                \\cos \\left( \\frac{1}{10000 \\frac{j-1}{d}} \\right) \\quad \\text{se } j \\text{ è dispari}\n",
    "            \\end{cases}\n",
    "        $$\n",
    "\n",
    "- A questo punto, dunque, l'input di un **multi-head attention** è la matrice $ \\mathbf{X} \\in \\mathbb{R}^{L \\times d} = \\mathbf{Z} + \\mathbf{P}$\n",
    "    - Questo layer calcola la Scaled Dot Product $h$ volte utilizzando pesi differenti, cioè proietto gli embeddings $h$ volte in maniera diversa ottenendo $h$ rappresentazioni diverse dell'input. Cioè la rete impara delle relazioni semantiche diverse in ognuno dei layer.\n",
    "    - Una volta calcolati questi $h$ risultati vengono riconcatenati e, attraverso un layer lineare, riportati alla dimensione originale $L \\times d$.\n",
    "    \n",
    "    <img src=\"imgs/multiheadatt.png\" alt=\"\" width=300>\n",
    "\n",
    "    - Per calcolare la Scaled Dot-Product Attention, è necessario avere tre matrici ottenute applicando le seguenti trasformazioni a $\\mathbf{X}$\n",
    "    1. key $\\mathbf{K} \\in \\mathbb{R}^{L \\times d_k} = \\mathbf{XW}_i^K, \\quad \\mathbf{W}_i^K \\in \\mathbb{R}^{d\\times d_k}$\n",
    "    2. query $\\mathbf{Q} \\in \\mathbb{R}^{L \\times d_k} = \\mathbf{XW}_i^Q, \\quad \\mathbf{W}_i^Q \\in \\mathbb{R}^{d\\times d_k}$\n",
    "    3. value $\\mathbf{V} \\in \\mathbb{R}^{L \\times d_v} = \\mathbf{XW}_i^V, \\quad \\mathbf{W}_i^V \\in \\mathbb{R}^{d\\times d_v}$\n",
    "    \n",
    "    Il pedice $i$ indica che questi calcoli vengono fatti per ogni layer $h$ di multi-head attention, $i = 0,1, \\dots, h-1$. Le dimensioni $d_k = d_v = \\frac{d}{h}$ sono scelte in modo che il prodotto matriciale sia possibile.\n",
    "\n",
    "    - Il calcolo della Scaled Dot-Product Attention è il seguente\n",
    "    $$\n",
    "    \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left( \\frac{\\mathbf{QK}^T}{\\sqrt{d_k}} \\right) \\mathbf{V}\n",
    "    $$\n",
    "    Questo calcola la relazione che c'è fra ogni token della frase con ogni altro. Ad esempio dicendo \"il gatto è nero e il cane abbaia\" il modello capisce che \"nero\" è un aggettivo che si riferisce al gatto e non al cane.\n",
    "\n",
    "\n",
    "    - Il costo computazionale per fare questa cosa è elevato poiché per ogni token devo calcolare la relazione con tutti gli altri. In particolare abbiamo \n",
    "        - in termini di tempo $O(L^2d)$\n",
    "        - in termini di spazio $O(L^2 + Ld)$\n",
    "\n",
    "\n",
    "## Bert Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "Dalla pubblicazione del modello **Transformer** nel 2017 si è capito quando fossero promettenti queste architetture. Il primo modello di successo effettivamente implementato, utilizzante questa architettura, è BERT.\n",
    "\n",
    "- BERT è un transformer addestrato appositamente per il NLP.\n",
    "- Gli autori introducono una fase di pre-addestramento self-supervised (senza bisogno di dati etichettati in modo da poter usare dataset enormi: corpus di miliardi di testi) su due task\n",
    "    1. **Masked Language Modelling**: dato un testo in input si mascherano (nascondono) alcune parole (circa il 15% delle parole) con un token speciale [MASK], si chiede al modello di ricostruire le parole mancanti. \n",
    "        - Per valutare l'accuratezza del modello si usa il **cross entropy loss**. Detto $M$ il numero di token mascherati, $t$ i vettori one hot encoded relativi alle parole corrette e $p$ la probabilita che il modello assegni alla parola corretta, allora la loss è data da\n",
    "        $$\n",
    "        L_{MLM} = -\\frac{1}{M} \\sum_{i=1}^M \\mathbf{t}_i \\log \\mathbf{p}_i\n",
    "        $$\n",
    "    2. **Next Sentence Prediction**: date due frasi in input, il modello deve capire se la seconda frase è la continuazione della prima. In questo caso l'errore è il **binary cross entropy loss**. Detta $y \\in (0,1)$ l'indicatore della classe reale e $p$ il valore predetto\n",
    "    $$\n",
    "    L_{NSP} = -y (\\log p + (1-y) \\log (1-p))\n",
    "    $$\n",
    "\n",
    "### BERT input\n",
    "- Rispetto all'input dei transformer cosi intesi come nel paper originale, BERT ha un input diverso. \n",
    "- dato un testo in input, il pre processing di questo testo per fornirlo in input al transformer è composto da tre embeddings\n",
    "    \n",
    "    - **Token embedding**: è l'embedding di ogni token della frase. Questo embedding è ottenuto da un embedding layer che prende in input il token e restituisce il suo embedding.\n",
    "    - **Sentence embedding**: poiché posso introdurre un testo intero, questo layer restituisce degli embeddings che indicano a che frase fa riferimento ogni token\n",
    "    - **Positional embedding**: come nel caso dei transformer, questo layer restituisce degli embeddings che indicano la posizione di ogni token nel testo.\n",
    "\n",
    "    \n",
    "    <img src=\"imgs/bertinput.PNG\" alt=\"\" width=500>\n",
    "\n",
    "### BERT Training\n",
    "- Hanno addestrato BERT su 800M di parole da BookCorpus e 2500M da Wikipedia.\n",
    "- Per la fase di tokenizzazione ed embedding è stato utilizzato WordPiece.\n",
    "- Le frasi avevano lunghezza $L$ di 512 token.\n",
    "- Sono state implementate due versioni di BERT a seconda dei parametri usati\n",
    "    - $\\text{BERT}_\\text{BASE}$: 12 layer, 768 hidden units, 12 attention heads, 110M parametri\n",
    "    - $\\text{BERT}_\\text{LARGE}$: 24 layer, 1024 hidden units, 16 attention heads, 340M parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning ed Information Retrieval\n",
    "- Il **metric learning** è un sottoinsieme del machine learning che si occupa di apprendere una funzione di distanza fra due oggetti.\n",
    "    - In altre parole se ho un insieme di dati, potessi trasformarli in vettori e inserirli in uno spazio vettoriale detto **latente** (cioè non osservabile) tale per cui \n",
    "        - oggetti semanticamente simili fra loro sono vicini fra loro\n",
    "        - dati dissimili vengono messi distanti\n",
    "    - Se si riesce a fare questo altri problemi di ML diventano molto più semplici\n",
    "        - la **classificazione** diventa un problema semplicissimo di nearest neighboor.\n",
    "        - l'**information retrieval** si semplifica. Se voglio tutti i dati rilevanti per una determinata query mi sarà sufficiente calcolare la distanza fra la query e tutti i dati e restituire i più vicini.\n",
    "- Esistono due tipologie di addestramenti, sulla base della nozione di **similarità semantica** (tipicamente etichette che classificano i dati in classi) fra dati\n",
    "    - **Supervised Metric Learning**: Ogni dato è etichettato e appartiene ad una specifica classe. L'obbiettivo è quello di apprendere una funzione di distanza che avvicini dati della stessa classe e allontani dati di classi diverse.\n",
    "    - **Weakly Supervised Metric Learning**: Non ho esattamente le etichette ma so che alcuni dati sono simili fra loro e altri no. Non so perché ma ho questa informazione. L'obbiettivo anche in questo caso rimane il medesimo\n",
    "\n",
    "## Come costruire questo spazio latente\n",
    "Occorre trovare un modello matematico che ci consenta di mappare i dati in uno spazio latente.\n",
    "Per fare questo si usano strumenti come distanza di Mahalanobis.\n",
    "\n",
    "### Distanza di Mahalanobis\n",
    "- In uno spazio $\\mathbb{R}^n$ per calcolare la vicinanza fra due vettori si usa la distanza euclidea.\n",
    "- Un'alternativa è la **distanza di Mahalanobis** che è una generalizzazione della distanza euclidea.\n",
    "    - Si vuole trovare una matrice di Mahalanobis $\\mathbf{M}$ tale che, presi due elementi simili $a$ (ancora, cioè valore di riferimento), $p$ (positivo, cioè un elemento simile all'ancora) e uno dissimile $n$ (negativo), si abbia $d_M(a,p) < d_M(a,n)$ con $d_M$ distanza di Mahalanobis definita cosi\n",
    "    $$\n",
    "    d_M(x,y) = \\sqrt{(x-y)^T \\mathbf{M} (x-y)}\n",
    "    $$\n",
    "    - Ogni matrice di Mhalaobis può anche essere scritta nella forma $M = W^TW$ (poiché $M$ è semidefinita positiva), allora si ha che\n",
    "    $$\n",
    "    d_M(x,y) = \\sqrt{(x-y)^T W^TW(x-y)} = \\sqrt{(Wx - Wy)^T(Wx - Wy)} = \\vert \\vert Wx - Wy \\vert \\vert_2\n",
    "    $$\n",
    "    - Nel **deep metric learning** si usano delle reti neruali profonde (come Transformers) con pesi addestrabili $\\theta$ che mi determinino una trasfmorazione $W_\\theta$. \n",
    "\n",
    "Vediamo ad esempio che \n",
    "- nella figura (a) abbiamo lo spazio originale con tre tipologie di figure geometriche. \n",
    "- Nella figura (b) capiamo che la distanza euclidea (caso specifico di Mahalanobis) calcola la distanza fra due figure.\n",
    "    - Il nostro obbiettivo è quello di \"forzare\" la distanza euclidea ad allontanare le figure dissimili e avvicinare quelle simili (punto (c)).\n",
    "    - Per fare questo possiamo fornire in input a delle reti neurali profonde le nostre figure che calcolano degli embeddings (punto (d)) $W$ e addestrarle a fare in modo che questi pesi siano minori per figure simili e maggiori per quelle dissimili.\n",
    "    - In questo modo la trasformazione $W$ ci consente di mappare i dati in uno spazio latente produce una suddivisione in classi simili degli oggetti (figura (e)).\n",
    "\n",
    "<img src=\"imgs/metriclearning.PNG\" alt=\"\" width=500>\n",
    "\n",
    "### Funzioni di Loss\n",
    "- Abbiamo visto che tramite degli embeddings possiamo mappare i dati in uno spazio latente\n",
    "    - in particolare calcolando una matrice di trasformazioni $W$ che ci consente di calcolare la distanza di Mahalanobis. \n",
    "    - Questa matrice è calcolata mediante **delle funzioni di loss** che forzino il modello ad avvicinare gli embedding di dati simili e allontanare quelli dissimili.\n",
    "- Esistono diverse funzioni di loss\n",
    "    - **Contrastive Loss**. Data una coppia di dati $(x_1, x_2)$ con rispettive etichette $y_1$ e $y_2$ che potrebbero essere ugali o meno (cioè non sappiamo se $y_1 = y_2$), la loss è definita come\n",
    "    $$\n",
    "        L = \\mathbb{I}_{y_1=y_2} \\vert \\vert W_{\\theta} x_1 - W_{\\theta}x_2 \\vert \\vert_2^2 + \\mathbb{I}_{y_1 \\neq y_2} \\max\\left(0, \\alpha - \\vert \\vert W_{\\theta} x_1 - W_{\\theta}x_2 \\vert \\vert_2^2 \\right)\n",
    "    $$\n",
    "    dove $\\mathbb{I}_A$ è la funzione indicatriceche vale 1 se la proposizione A è vera, 0 altrimenti. \n",
    "    - Pertanto la loss è composta da due termini\n",
    "        - il primo termine è la distanza euclidea fra i due embedding, moltiplicata per un fattore $\\mathbb{I}_{y_1=y_2} \\neq 0$ solo se i due dati sono simili.\n",
    "        - il secondo termine è il massimo fra 0 e la distanza euclidea fra i due embedding se i due dati sono dissimili.\n",
    "    - Intuitivamente poiché voglio minimizzare la loss $L$ \n",
    "        - se $L = $ distanza tra due embeddings simili, minimizzandola otterrò una distanza minore fra i due embedding\n",
    "        - se $L = $ distanza tra due embeddings dissimili, minimizzandola otterrò una distanza maggiore fra i due embedding. Questo perché tanto più vicini sono i due elementi nello spazio originale, quanto più i loro embeddings saranno distanti.\n",
    "        Infatti la distanza euclidea al quadrato ($\\vert \\vert \\cdot \\vert \\vert_2^2$) sarà molto prossima a 0 e dunque $\\alpha - \\vert \\vert \\cdot \\vert \\vert_2^2$ sarà molto grande, e poiché devo prendere il massimo fra quello e 0 prenderò quello. \n",
    "            - $\\alpha$ è un iperparametro che indica la distanza minima che voglio fra due embedding di elementi dissimili. Tipicamente uguale a 1 o 0.1.\n",
    "    - **Triplete Loss**. Funzione molto più efficiente, di contrastive loss (in essa infatti si confrontavano sempre due coppie di elementi, dunque si andava a visualizzare la distanza relativa fra i due elementi). In questo caso, infatti, piuttosto che considerare coppie di elementi, si agisce su delle triplette $x_a, x_p, x_n$ con $y_a = y_p \\neq y_n$ (cioè i primi due sono simili mentre primo e terzo no).\n",
    "        - Si cerca di minimizzare una loss fatta cosi\n",
    "        $$\n",
    "        L = \\text{max}\\left( 0, \\vert \\vert W_\\theta x_a - W_\\theta x_p \\vert \\vert_2^2 - \\vert \\vert W_\\theta x_a - W_\\theta x_n \\vert \\vert_2^2 + \\alpha \\right)\n",
    "        $$\n",
    "        - Due elementi sono considerati dissimili se la loro loss è maggiore di $\\alpha$.\n",
    "\n",
    "## Information Retrieval con Metric Learning\n",
    "- L'obbiettivo dell'information retrieval è quello di trovare i documenti più rilevanti per una determinata query. \n",
    "- Formalmente data una query $S \\in \\mathbb{S}$ tra un insieme di documenti $\\mathbb{T} = \\left\\{ t_1, t_2, \\dots, t_n \\right\\}$\n",
    "    - È necessario trovare una funzione **di ranking** $f(s,t)$ che restituisca uno score di similarità tra una query e un documento.\n",
    "    - Tipicamente si ha $f(s,t) = g(\\psi(s), \\phi(t), \\eta(s,t))$ dove\n",
    "        - $\\psi, \\phi$ sono funzioni che estraggono le feature rispettivamente di query e documenti\n",
    "        - $\\eta$ è una funzione di interazione che modella le relazioni fra query e documento\n",
    "    - Normalmente $\\psi, \\phi, \\eta$ sono transformers (dunque definiti manualmente) e $g$ è un modello di machine learning.\n",
    "\n",
    "    1:05:13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
